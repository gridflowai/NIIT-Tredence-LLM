{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b45ee2-a664-4ee3-b4a9-882ba6dd4acf",
   "metadata": {},
   "source": [
    "#### Intro on Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027450aa-3bec-4cfc-ab6f-c8ba369c2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain\n",
    "#pip install langchain-core\n",
    "#pip install langchain-openai\n",
    "#pip install langchain-community\n",
    "#pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e615116-0b86-4224-b793-788b9af12fc7",
   "metadata": {},
   "source": [
    "- **Large Language Models (LLMs)**:\n",
    "  - Powerful machine learning models adept at various language-related tasks, including:\n",
    "    - Text generation\n",
    "    - Translation\n",
    "    - Summarization\n",
    "    - Question answering\n",
    "  - Capable of performing these tasks without specific fine-tuning for each scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d974c-cbf3-4bb4-b1a1-9b4384b96367",
   "metadata": {},
   "source": [
    "- **Chat Model Interface**:\n",
    "  - Commonly used to interact with modern LLMs.\n",
    "  - Accepts a list of messages as input and returns a single message as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063ad72-26a3-4a98-94bb-fa7fe45db7de",
   "metadata": {},
   "source": [
    "- **New Features in Chat Models**:\n",
    "  - **Tool Calling**:\n",
    "    - Many chat models now support a native tool-calling API.\n",
    "    - Allows the model to interact with external services, APIs, and databases.\n",
    "    - Useful for extracting structured information from unstructured data.\n",
    "    - Enables the creation of advanced applications with AI-driven interactions.\n",
    "  - - **Structured Output**:\n",
    "    - Enables chat models to respond in specific formats, such as JSON.\n",
    "    - Useful for producing outputs that follow a predefined schema.\n",
    "  - - **Multimodality**:\n",
    "    - Extends the model’s capabilities beyond text processing.\n",
    "    - Supports handling of diverse data types, including images, audio, and video, broadening the range of tasks it can perform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9463a6-9c2e-414f-a168-91bde23d0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a168de-bc2c-46e7-8dae-0ece95f80dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd67c27-dd30-4ab7-8606-83e4e2d59e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user question\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b280bc84-106b-4a9a-885b-e8c0d7207443",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697109da-aad4-4d58-b89a-57ccd04c9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "        template       = template,\n",
    "        input_variables= ['question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0455a559-0993-4860-8fdf-3ce605c4ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "                model_name='gpt-4o-mini'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6b74e9-585d-49ba-9406-06cd6020e923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhupe\\AppData\\Local\\Temp\\ipykernel_21768\\3584584265.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt = prompt,\n",
    "    llm    = model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3dd253d-fd3d-4347-93d5-038900443a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhupe\\AppData\\Local\\Temp\\ipykernel_21768\\3941736507.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm_chain.run(question))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New Orleans Saints won the Super Bowl in the 2010 season. They defeated the Indianapolis Colts in Super Bowl XLIV, which took place on February 7, 2010.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f97a4-f889-4b4c-940b-dd468dbb8522",
   "metadata": {},
   "source": [
    "##### multiple questions using generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b87b95-8bb6-4e8b-b33f-774176b65b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef5a5c7-d719-4043-a531-e3cc721a682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:369: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='The New Orleans Saints won the Super Bowl in the 2010 season, defeating the Indianapolis Colts in Super Bowl XLIV.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='The New Orleans Saints won the Super Bowl in the 2010 season, defeating the Indianapolis Colts in Super Bowl XLIV.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 27, 'total_tokens': 52, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-6f0293f6-e82b-4871-90d7-fa4df9d889e7-0', usage_metadata={'input_tokens': 27, 'output_tokens': 25, 'total_tokens': 52, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}))], [ChatGeneration(text='To convert height from feet and inches to centimeters, you can use the following conversions:\\n\\n1 foot = 30.48 centimeters  \\n1 inch = 2.54 centimeters  \\n\\nFirst, convert the feet to centimeters:\\n6 feet × 30.48 cm/foot = 182.88 cm\\n\\nNext, convert the inches to centimeters:\\n4 inches × 2.54 cm/inch = 10.16 cm\\n\\nNow, add the two results together:\\n182.88 cm + 10.16 cm = 193.04 cm\\n\\nSo, if you are 6 feet 4 inches tall, you are approximately **193.04 centimeters** tall.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='To convert height from feet and inches to centimeters, you can use the following conversions:\\n\\n1 foot = 30.48 centimeters  \\n1 inch = 2.54 centimeters  \\n\\nFirst, convert the feet to centimeters:\\n6 feet × 30.48 cm/foot = 182.88 cm\\n\\nNext, convert the inches to centimeters:\\n4 inches × 2.54 cm/inch = 10.16 cm\\n\\nNow, add the two results together:\\n182.88 cm + 10.16 cm = 193.04 cm\\n\\nSo, if you are 6 feet 4 inches tall, you are approximately **193.04 centimeters** tall.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 30, 'total_tokens': 164, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7dce6e1-651a-4982-b00a-4c96cbfb1e0c-0', usage_metadata={'input_tokens': 30, 'output_tokens': 134, 'total_tokens': 164, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}))], [ChatGeneration(text='The 12th person to walk on the Moon was Charles \"Charlie\" Duke. He was the lunar module pilot for the Apollo 16 mission, which landed on the Moon in April 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='The 12th person to walk on the Moon was Charles \"Charlie\" Duke. He was the lunar module pilot for the Apollo 16 mission, which landed on the Moon in April 1972.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 24, 'total_tokens': 65, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_9b78b61c52', 'finish_reason': 'stop', 'logprobs': None}, id='run-44b28c80-8d80-4bea-b9c5-f806feda71fd-0', usage_metadata={'input_tokens': 24, 'output_tokens': 41, 'total_tokens': 65, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}))], [ChatGeneration(text='A blade of grass does not have any eyes. Grass is a plant and does not possess sensory organs like eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='A blade of grass does not have any eyes. Grass is a plant and does not possess sensory organs like eyes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 23, 'total_tokens': 46, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-73c38327-0342-461d-8c84-2c3f5c7b643a-0', usage_metadata={'input_tokens': 23, 'output_tokens': 23, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}))]], llm_output={'token_usage': {'completion_tokens': 223, 'prompt_tokens': 104, 'total_tokens': 327, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_0ba0d124f1'}, run=[RunInfo(run_id=UUID('6f0293f6-e82b-4871-90d7-fa4df9d889e7')), RunInfo(run_id=UUID('f7dce6e1-651a-4982-b00a-4c96cbfb1e0c')), RunInfo(run_id=UUID('44b28c80-8d80-4bea-b9c5-f806feda71fd')), RunInfo(run_id=UUID('73c38327-0342-461d-8c84-2c3f5c7b643a'))], type='LLMResult')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_chain.generate(qs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f34fca-a4c9-49d7-a4a6-c2b2ebbf8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "The New Orleans Saints won the Super Bowl in the 2010 season, defeating the Indianapolis Colts in Super Bowl XLIV.\n",
      "\n",
      "Response 2:\n",
      "To convert height from feet and inches to centimeters, you can use the following conversions:\n",
      "\n",
      "1 foot = 30.48 centimeters  \n",
      "1 inch = 2.54 centimeters  \n",
      "\n",
      "First, convert the feet to centimeters:\n",
      "6 feet × 30.48 cm/foot = 182.88 cm\n",
      "\n",
      "Next, convert the inches to centimeters:\n",
      "4 inches × 2.54 cm/inch = 10.16 cm\n",
      "\n",
      "Now, add the two results together:\n",
      "182.88 cm + 10.16 cm = 193.04 cm\n",
      "\n",
      "So, if you are 6 feet 4 inches tall, you are approximately **193.04 centimeters** tall.\n",
      "\n",
      "Response 3:\n",
      "The 12th person to walk on the Moon was Charles \"Charlie\" Duke. He was the lunar module pilot for the Apollo 16 mission, which landed on the Moon in April 1972.\n",
      "\n",
      "Response 4:\n",
      "A blade of grass does not have any eyes. Grass is a plant and does not possess sensory organs like eyes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the generations and print each response\n",
    "for i, generation in enumerate(result.generations):\n",
    "    print(f\"Response {i+1}:\")\n",
    "    print(generation[0].text)  # Access the text of the first generation in each group\n",
    "    print()  # Add a newline for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33c9d8-c1ce-435d-88b8-484185de2f3d",
   "metadata": {},
   "source": [
    "#### Key Methods of a Chat Model\n",
    "\n",
    "- **invoke**: This is the main method used to interact with a chat model. It accepts a list of messages as input and produces a list of messages as output.\n",
    "- **stream**: This method enables the streaming of the chat model's output as it is being generated, allowing for real-time responses.\n",
    "- **batch**: This method allows for the batching of multiple requests to the chat model, improving processing efficiency by handling multiple inputs at once.\n",
    "- **bind_tools**: This method facilitates the binding of tools to the chat model, making them available within the execution context of the model.\n",
    "- **with_structured_output**: This is a wrapper around the invoke method designed for models that inherently support structured output, enhancing the output formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac73c6a-0a46-4a41-876b-2296f3d502d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
