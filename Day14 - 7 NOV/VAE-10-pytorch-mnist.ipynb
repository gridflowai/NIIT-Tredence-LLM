{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd372cc3-a218-45ac-ac94-d8a36293ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c208eb-49b4-430e-87c5-f6137d02f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6bf598-737d-4e36-b272-b9eba16387e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc21(h)\n",
    "        log_var = self.fc22(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    # def decode(self, z):\n",
    "    #     h = F.relu(self.fc3(z))\n",
    "    #     return torch.sigmoid(self.fc4(h))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h))  # Ensure output is in [0, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b744bee9-4b34-4032-854f-67c015c53085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6645d7c-290e-4a4c-9574-e37dbd79e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_dim = 784  # Assuming MNIST-like dataset\n",
    "hidden_dim = 256\n",
    "latent_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1690a813-2396-4f54-80b1-742e840db690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VAE\n",
    "vae = VAE(input_dim, hidden_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4845c8ba-4aff-4a07-a944-f16cc53454ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0198effc-0f4b-4784-98ce-c1ccd2faee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c3e5e5-f315-4266-b7f9-c68f90b56a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for MNIST\n",
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                       # Converts [0, 255] to [0.0, 1.0]\n",
    "    #transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20e1c14c-4e1c-4c29-869f-ca2c38f00105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [0/469], Loss: 545.6916\n",
      "Epoch [1/50], Step [100/469], Loss: 194.9076\n",
      "Epoch [1/50], Step [200/469], Loss: 164.1754\n",
      "Epoch [1/50], Step [300/469], Loss: 161.9988\n",
      "Epoch [1/50], Step [400/469], Loss: 145.6302\n",
      "Epoch [2/50], Step [0/469], Loss: 138.6867\n",
      "Epoch [2/50], Step [100/469], Loss: 134.8753\n",
      "Epoch [2/50], Step [200/469], Loss: 136.1489\n",
      "Epoch [2/50], Step [300/469], Loss: 132.1799\n",
      "Epoch [2/50], Step [400/469], Loss: 123.6593\n",
      "Epoch [3/50], Step [0/469], Loss: 129.8332\n",
      "Epoch [3/50], Step [100/469], Loss: 118.4102\n",
      "Epoch [3/50], Step [200/469], Loss: 118.5678\n",
      "Epoch [3/50], Step [300/469], Loss: 121.3571\n",
      "Epoch [3/50], Step [400/469], Loss: 122.5141\n",
      "Epoch [4/50], Step [0/469], Loss: 124.2179\n",
      "Epoch [4/50], Step [100/469], Loss: 120.5249\n",
      "Epoch [4/50], Step [200/469], Loss: 115.2439\n",
      "Epoch [4/50], Step [300/469], Loss: 114.7247\n",
      "Epoch [4/50], Step [400/469], Loss: 114.8214\n",
      "Epoch [5/50], Step [0/469], Loss: 114.6236\n",
      "Epoch [5/50], Step [100/469], Loss: 113.7574\n",
      "Epoch [5/50], Step [200/469], Loss: 112.8244\n",
      "Epoch [5/50], Step [300/469], Loss: 115.2934\n",
      "Epoch [5/50], Step [400/469], Loss: 112.8126\n",
      "Epoch [6/50], Step [0/469], Loss: 111.4390\n",
      "Epoch [6/50], Step [100/469], Loss: 112.3010\n",
      "Epoch [6/50], Step [200/469], Loss: 110.9310\n",
      "Epoch [6/50], Step [300/469], Loss: 116.4404\n",
      "Epoch [6/50], Step [400/469], Loss: 111.0185\n",
      "Epoch [7/50], Step [0/469], Loss: 109.5153\n",
      "Epoch [7/50], Step [100/469], Loss: 108.6784\n",
      "Epoch [7/50], Step [200/469], Loss: 108.5081\n",
      "Epoch [7/50], Step [300/469], Loss: 112.7987\n",
      "Epoch [7/50], Step [400/469], Loss: 109.7337\n",
      "Epoch [8/50], Step [0/469], Loss: 111.3352\n",
      "Epoch [8/50], Step [100/469], Loss: 107.7910\n",
      "Epoch [8/50], Step [200/469], Loss: 107.9770\n",
      "Epoch [8/50], Step [300/469], Loss: 109.2528\n",
      "Epoch [8/50], Step [400/469], Loss: 106.2903\n",
      "Epoch [9/50], Step [0/469], Loss: 109.0959\n",
      "Epoch [9/50], Step [100/469], Loss: 109.6829\n",
      "Epoch [9/50], Step [200/469], Loss: 112.8698\n",
      "Epoch [9/50], Step [300/469], Loss: 107.7058\n",
      "Epoch [9/50], Step [400/469], Loss: 113.8732\n",
      "Epoch [10/50], Step [0/469], Loss: 111.1520\n",
      "Epoch [10/50], Step [100/469], Loss: 107.8390\n",
      "Epoch [10/50], Step [200/469], Loss: 110.6019\n",
      "Epoch [10/50], Step [300/469], Loss: 111.4084\n",
      "Epoch [10/50], Step [400/469], Loss: 108.6624\n",
      "Epoch [11/50], Step [0/469], Loss: 107.5957\n",
      "Epoch [11/50], Step [100/469], Loss: 101.7299\n",
      "Epoch [11/50], Step [200/469], Loss: 110.4117\n",
      "Epoch [11/50], Step [300/469], Loss: 106.2112\n",
      "Epoch [11/50], Step [400/469], Loss: 107.5972\n",
      "Epoch [12/50], Step [0/469], Loss: 109.1727\n",
      "Epoch [12/50], Step [100/469], Loss: 107.8157\n",
      "Epoch [12/50], Step [200/469], Loss: 106.3848\n",
      "Epoch [12/50], Step [300/469], Loss: 104.6971\n",
      "Epoch [12/50], Step [400/469], Loss: 108.2733\n",
      "Epoch [13/50], Step [0/469], Loss: 109.6082\n",
      "Epoch [13/50], Step [100/469], Loss: 108.9163\n",
      "Epoch [13/50], Step [200/469], Loss: 104.2715\n",
      "Epoch [13/50], Step [300/469], Loss: 103.8155\n",
      "Epoch [13/50], Step [400/469], Loss: 104.4096\n",
      "Epoch [14/50], Step [0/469], Loss: 106.3873\n",
      "Epoch [14/50], Step [100/469], Loss: 108.1928\n",
      "Epoch [14/50], Step [200/469], Loss: 107.8482\n",
      "Epoch [14/50], Step [300/469], Loss: 108.6214\n",
      "Epoch [14/50], Step [400/469], Loss: 107.4872\n",
      "Epoch [15/50], Step [0/469], Loss: 108.2376\n",
      "Epoch [15/50], Step [100/469], Loss: 108.8940\n",
      "Epoch [15/50], Step [200/469], Loss: 105.4748\n",
      "Epoch [15/50], Step [300/469], Loss: 106.0057\n",
      "Epoch [15/50], Step [400/469], Loss: 107.7063\n",
      "Epoch [16/50], Step [0/469], Loss: 105.9529\n",
      "Epoch [16/50], Step [100/469], Loss: 106.3584\n",
      "Epoch [16/50], Step [200/469], Loss: 103.6216\n",
      "Epoch [16/50], Step [300/469], Loss: 105.0426\n",
      "Epoch [16/50], Step [400/469], Loss: 114.6878\n",
      "Epoch [17/50], Step [0/469], Loss: 109.2422\n",
      "Epoch [17/50], Step [100/469], Loss: 104.4404\n",
      "Epoch [17/50], Step [200/469], Loss: 107.4025\n",
      "Epoch [17/50], Step [300/469], Loss: 106.6821\n",
      "Epoch [17/50], Step [400/469], Loss: 106.5349\n",
      "Epoch [18/50], Step [0/469], Loss: 105.5789\n",
      "Epoch [18/50], Step [100/469], Loss: 107.9036\n",
      "Epoch [18/50], Step [200/469], Loss: 108.5387\n",
      "Epoch [18/50], Step [300/469], Loss: 105.8515\n",
      "Epoch [18/50], Step [400/469], Loss: 105.6964\n",
      "Epoch [19/50], Step [0/469], Loss: 102.5068\n",
      "Epoch [19/50], Step [100/469], Loss: 106.3626\n",
      "Epoch [19/50], Step [200/469], Loss: 103.7771\n",
      "Epoch [19/50], Step [300/469], Loss: 105.4249\n",
      "Epoch [19/50], Step [400/469], Loss: 107.3580\n",
      "Epoch [20/50], Step [0/469], Loss: 104.0915\n",
      "Epoch [20/50], Step [100/469], Loss: 103.9459\n",
      "Epoch [20/50], Step [200/469], Loss: 109.7872\n",
      "Epoch [20/50], Step [300/469], Loss: 103.4808\n",
      "Epoch [20/50], Step [400/469], Loss: 104.8309\n",
      "Epoch [21/50], Step [0/469], Loss: 103.0276\n",
      "Epoch [21/50], Step [100/469], Loss: 104.2537\n",
      "Epoch [21/50], Step [200/469], Loss: 102.8241\n",
      "Epoch [21/50], Step [300/469], Loss: 102.6384\n",
      "Epoch [21/50], Step [400/469], Loss: 106.2932\n",
      "Epoch [22/50], Step [0/469], Loss: 103.9505\n",
      "Epoch [22/50], Step [100/469], Loss: 103.8732\n",
      "Epoch [22/50], Step [200/469], Loss: 102.4667\n",
      "Epoch [22/50], Step [300/469], Loss: 106.9493\n",
      "Epoch [22/50], Step [400/469], Loss: 102.0479\n",
      "Epoch [23/50], Step [0/469], Loss: 102.3417\n",
      "Epoch [23/50], Step [100/469], Loss: 107.1799\n",
      "Epoch [23/50], Step [200/469], Loss: 106.1349\n",
      "Epoch [23/50], Step [300/469], Loss: 107.3886\n",
      "Epoch [23/50], Step [400/469], Loss: 104.5838\n",
      "Epoch [24/50], Step [0/469], Loss: 104.7436\n",
      "Epoch [24/50], Step [100/469], Loss: 108.9802\n",
      "Epoch [24/50], Step [200/469], Loss: 106.2871\n",
      "Epoch [24/50], Step [300/469], Loss: 104.2422\n",
      "Epoch [24/50], Step [400/469], Loss: 105.1933\n",
      "Epoch [25/50], Step [0/469], Loss: 103.0438\n",
      "Epoch [25/50], Step [100/469], Loss: 106.2544\n",
      "Epoch [25/50], Step [200/469], Loss: 108.1962\n",
      "Epoch [25/50], Step [300/469], Loss: 106.1555\n",
      "Epoch [25/50], Step [400/469], Loss: 104.2335\n",
      "Epoch [26/50], Step [0/469], Loss: 102.3458\n",
      "Epoch [26/50], Step [100/469], Loss: 106.7263\n",
      "Epoch [26/50], Step [200/469], Loss: 107.9442\n",
      "Epoch [26/50], Step [300/469], Loss: 103.8127\n",
      "Epoch [26/50], Step [400/469], Loss: 103.8164\n",
      "Epoch [27/50], Step [0/469], Loss: 103.8398\n",
      "Epoch [27/50], Step [100/469], Loss: 107.1800\n",
      "Epoch [27/50], Step [200/469], Loss: 107.4946\n",
      "Epoch [27/50], Step [300/469], Loss: 102.5390\n",
      "Epoch [27/50], Step [400/469], Loss: 103.7073\n",
      "Epoch [28/50], Step [0/469], Loss: 104.4332\n",
      "Epoch [28/50], Step [100/469], Loss: 103.3048\n",
      "Epoch [28/50], Step [200/469], Loss: 105.1800\n",
      "Epoch [28/50], Step [300/469], Loss: 105.4122\n",
      "Epoch [28/50], Step [400/469], Loss: 105.3483\n",
      "Epoch [29/50], Step [0/469], Loss: 105.0026\n",
      "Epoch [29/50], Step [100/469], Loss: 104.1180\n",
      "Epoch [29/50], Step [200/469], Loss: 103.6221\n",
      "Epoch [29/50], Step [300/469], Loss: 107.2606\n",
      "Epoch [29/50], Step [400/469], Loss: 104.8138\n",
      "Epoch [30/50], Step [0/469], Loss: 107.6933\n",
      "Epoch [30/50], Step [100/469], Loss: 107.8604\n",
      "Epoch [30/50], Step [200/469], Loss: 104.1478\n",
      "Epoch [30/50], Step [300/469], Loss: 108.6303\n",
      "Epoch [30/50], Step [400/469], Loss: 106.4634\n",
      "Epoch [31/50], Step [0/469], Loss: 105.1909\n",
      "Epoch [31/50], Step [100/469], Loss: 100.3090\n",
      "Epoch [31/50], Step [200/469], Loss: 106.3815\n",
      "Epoch [31/50], Step [300/469], Loss: 105.7186\n",
      "Epoch [31/50], Step [400/469], Loss: 104.3470\n",
      "Epoch [32/50], Step [0/469], Loss: 102.8373\n",
      "Epoch [32/50], Step [100/469], Loss: 102.5698\n",
      "Epoch [32/50], Step [200/469], Loss: 104.4761\n",
      "Epoch [32/50], Step [300/469], Loss: 104.9103\n",
      "Epoch [32/50], Step [400/469], Loss: 105.7039\n",
      "Epoch [33/50], Step [0/469], Loss: 107.0569\n",
      "Epoch [33/50], Step [100/469], Loss: 105.7349\n",
      "Epoch [33/50], Step [200/469], Loss: 104.9891\n",
      "Epoch [33/50], Step [300/469], Loss: 104.5123\n",
      "Epoch [33/50], Step [400/469], Loss: 103.3488\n",
      "Epoch [34/50], Step [0/469], Loss: 103.7436\n",
      "Epoch [34/50], Step [100/469], Loss: 102.5709\n",
      "Epoch [34/50], Step [200/469], Loss: 103.5038\n",
      "Epoch [34/50], Step [300/469], Loss: 103.8629\n",
      "Epoch [34/50], Step [400/469], Loss: 104.4930\n",
      "Epoch [35/50], Step [0/469], Loss: 104.6274\n",
      "Epoch [35/50], Step [100/469], Loss: 103.4560\n",
      "Epoch [35/50], Step [200/469], Loss: 104.7143\n",
      "Epoch [35/50], Step [300/469], Loss: 104.1905\n",
      "Epoch [35/50], Step [400/469], Loss: 106.3252\n",
      "Epoch [36/50], Step [0/469], Loss: 103.3653\n",
      "Epoch [36/50], Step [100/469], Loss: 102.8274\n",
      "Epoch [36/50], Step [200/469], Loss: 104.1415\n",
      "Epoch [36/50], Step [300/469], Loss: 103.0396\n",
      "Epoch [36/50], Step [400/469], Loss: 106.9612\n",
      "Epoch [37/50], Step [0/469], Loss: 108.7584\n",
      "Epoch [37/50], Step [100/469], Loss: 105.1730\n",
      "Epoch [37/50], Step [200/469], Loss: 105.6107\n",
      "Epoch [37/50], Step [300/469], Loss: 100.3281\n",
      "Epoch [37/50], Step [400/469], Loss: 106.8443\n",
      "Epoch [38/50], Step [0/469], Loss: 100.4451\n",
      "Epoch [38/50], Step [100/469], Loss: 108.0412\n",
      "Epoch [38/50], Step [200/469], Loss: 98.4437\n",
      "Epoch [38/50], Step [300/469], Loss: 103.5773\n",
      "Epoch [38/50], Step [400/469], Loss: 105.8111\n",
      "Epoch [39/50], Step [0/469], Loss: 104.7856\n",
      "Epoch [39/50], Step [100/469], Loss: 106.6422\n",
      "Epoch [39/50], Step [200/469], Loss: 103.0854\n",
      "Epoch [39/50], Step [300/469], Loss: 102.3319\n",
      "Epoch [39/50], Step [400/469], Loss: 100.9558\n",
      "Epoch [40/50], Step [0/469], Loss: 101.3679\n",
      "Epoch [40/50], Step [100/469], Loss: 104.8694\n",
      "Epoch [40/50], Step [200/469], Loss: 104.4729\n",
      "Epoch [40/50], Step [300/469], Loss: 104.5861\n",
      "Epoch [40/50], Step [400/469], Loss: 106.3108\n",
      "Epoch [41/50], Step [0/469], Loss: 102.2344\n",
      "Epoch [41/50], Step [100/469], Loss: 105.1762\n",
      "Epoch [41/50], Step [200/469], Loss: 105.0820\n",
      "Epoch [41/50], Step [300/469], Loss: 102.9764\n",
      "Epoch [41/50], Step [400/469], Loss: 105.3490\n",
      "Epoch [42/50], Step [0/469], Loss: 103.4100\n",
      "Epoch [42/50], Step [100/469], Loss: 104.1876\n",
      "Epoch [42/50], Step [200/469], Loss: 107.7995\n",
      "Epoch [42/50], Step [300/469], Loss: 99.9286\n",
      "Epoch [42/50], Step [400/469], Loss: 103.1629\n",
      "Epoch [43/50], Step [0/469], Loss: 105.9553\n",
      "Epoch [43/50], Step [100/469], Loss: 100.2604\n",
      "Epoch [43/50], Step [200/469], Loss: 106.2067\n",
      "Epoch [43/50], Step [300/469], Loss: 103.5259\n",
      "Epoch [43/50], Step [400/469], Loss: 100.9289\n",
      "Epoch [44/50], Step [0/469], Loss: 106.0128\n",
      "Epoch [44/50], Step [100/469], Loss: 99.4701\n",
      "Epoch [44/50], Step [200/469], Loss: 108.6929\n",
      "Epoch [44/50], Step [300/469], Loss: 103.0226\n",
      "Epoch [44/50], Step [400/469], Loss: 102.0642\n",
      "Epoch [45/50], Step [0/469], Loss: 106.1421\n",
      "Epoch [45/50], Step [100/469], Loss: 104.0717\n",
      "Epoch [45/50], Step [200/469], Loss: 99.8453\n",
      "Epoch [45/50], Step [300/469], Loss: 108.6834\n",
      "Epoch [45/50], Step [400/469], Loss: 102.8656\n",
      "Epoch [46/50], Step [0/469], Loss: 102.5355\n",
      "Epoch [46/50], Step [100/469], Loss: 103.1062\n",
      "Epoch [46/50], Step [200/469], Loss: 105.2847\n",
      "Epoch [46/50], Step [300/469], Loss: 106.1099\n",
      "Epoch [46/50], Step [400/469], Loss: 105.9345\n",
      "Epoch [47/50], Step [0/469], Loss: 100.6666\n",
      "Epoch [47/50], Step [100/469], Loss: 102.7723\n",
      "Epoch [47/50], Step [200/469], Loss: 106.0064\n",
      "Epoch [47/50], Step [300/469], Loss: 101.6005\n",
      "Epoch [47/50], Step [400/469], Loss: 101.2761\n",
      "Epoch [48/50], Step [0/469], Loss: 99.1974\n",
      "Epoch [48/50], Step [100/469], Loss: 106.0700\n",
      "Epoch [48/50], Step [200/469], Loss: 105.0643\n",
      "Epoch [48/50], Step [300/469], Loss: 104.0027\n",
      "Epoch [48/50], Step [400/469], Loss: 103.6471\n",
      "Epoch [49/50], Step [0/469], Loss: 103.5803\n",
      "Epoch [49/50], Step [100/469], Loss: 98.5232\n",
      "Epoch [49/50], Step [200/469], Loss: 100.4211\n",
      "Epoch [49/50], Step [300/469], Loss: 106.1577\n",
      "Epoch [49/50], Step [400/469], Loss: 102.4662\n",
      "Epoch [50/50], Step [0/469], Loss: 101.1948\n",
      "Epoch [50/50], Step [100/469], Loss: 102.5800\n",
      "Epoch [50/50], Step [200/469], Loss: 106.2245\n",
      "Epoch [50/50], Step [300/469], Loss: 103.1351\n",
      "Epoch [50/50], Step [400/469], Loss: 103.6984\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        # Flatten the input data\n",
    "        x = data.view(-1, input_dim)\n",
    "\n",
    "        # Forward pass\n",
    "        recon_x, mu, log_var = vae(x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = vae_loss(recon_x, x, mu, log_var)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item()/batch_size:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98365eb-610f-4460-bf6e-f2b14e9c3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0a198a3-ad98-474b-871d-9022dd10ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(vae, dataloader):\n",
    "    latents = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data = data.view(-1, input_dim)\n",
    "            _, mu, _ = vae(data)\n",
    "            latents.append(mu)\n",
    "            labels.append(target)\n",
    "        latents = torch.cat(latents, dim=0)\n",
    "        labels  = torch.cat(labels, dim=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='rainbow')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.title('Latent Space Visualization')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb707b5-40c4-493d-ba2a-3326a11cf7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(vae, num_samples):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        samples = vae.decode(z)\n",
    "\n",
    "    samples = samples.view(-1, 1, 28, 28)  # Reshape samples if needed\n",
    "    samples = samples * 0.5 + 0.5  # Denormalize if needed\n",
    "\n",
    "    grid = torchvision.utils.make_grid(samples, nrow=int(num_samples ** 0.5), padding=2)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.title('Randomly Generated Samples')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cc8ebc0-2305-4446-b452-f7ff5bd12bc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_latent_space' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot latent space\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plot_latent_space(vae, dataloader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_latent_space' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot latent space\n",
    "plot_latent_space(vae, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb49472d-ef54-4167-994e-1ad7277dd9df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate samples\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m----> 3\u001b[0m generate_samples(vae, num_samples)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate samples\n",
    "num_samples = 25\n",
    "generate_samples(vae, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a84a9a-e558-4569-8372-ce3eee78cc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
