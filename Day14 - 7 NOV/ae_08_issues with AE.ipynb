{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4436df59-f409-44c4-bcc8-8ac7d6c58cac",
   "metadata": {},
   "source": [
    "## Issues with Autoencoders\n",
    "\n",
    "#### 1. Lack of Robust Latent Space Structure\n",
    "\n",
    "- **Traditional AE Issue**:\n",
    "\n",
    "    - In a traditional AE, the encoder compresses data into a latent space, but there is `no guarantee` that this space is `structured` or `organized in a meaningful way`.\n",
    "    - This disorganized latent space makes it challenging to sample meaningful new data points, as the space doesn’t capture the data distribution well.\n",
    "\n",
    "- **VAE Solution**:\n",
    "\n",
    "    - VAEs introduce a probabilistic approach by encoding `data as distributions` rather than fixed points in the latent space.\n",
    "    - Instead of mapping each input to a single point, a VAE maps it to a **mean** and **variance**.\n",
    "    - During training, the VAE learns to model the latent space as a `continuous and structured space by encouraging it to approximate a known distribution, typically a **standard normal distribution**.\n",
    "    - This makes it possible to sample coherent new data from any point within the distribution, leading to a smoother and more organized latent space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b364b75-6ace-4d26-818d-2c9d1eeabdd3",
   "metadata": {},
   "source": [
    "#### 2. Difficulty in Sampling New Data\n",
    "\n",
    "- **Traditional AE Issue**:\n",
    "\n",
    "    - Since traditional AEs map data to specific points in an unstructured latent space, `random sampling` in this space often leads to `meaningless outputs` that don’t resemble the training data.\n",
    "\n",
    "- **VAE Solution**:\n",
    "\n",
    "    - VAEs address this with **reparameterization** and **probabilistic sampling**.\n",
    "    - By sampling from the learned distribution, we can generate new points in the latent space that are close to real data points.\n",
    "    - The **reparameterization trick**, enables VAEs to perform stochastic sampling in a way that’s compatible with gradient-based learning.\n",
    "    - This approach makes it easier to sample new data points that closely resemble the original data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0732a-4e3b-474b-a4bf-62f5f607bb06",
   "metadata": {},
   "source": [
    "#### 3. Inflexible and Deterministic Output\n",
    "\n",
    "- **Traditional AE Issue**:\n",
    "\n",
    "    - Traditional AEs are **deterministic**, meaning for any input, they always produce the same output in the latent space.\n",
    "    - This approach limits the AE’s ability to generalize to minor variations in the data and doesn’t allow for **variability** in generated outputs.\n",
    "\n",
    "- **VAE Solution**:\n",
    "\n",
    "    - VAEs introduce **variability** by encoding data as distributions (mean and variance) rather than single points.\n",
    "    - For a given input, VAEs can output slightly different reconstructed data by sampling from the distribution.\n",
    "    - This added **variability** enhances the model’s robustness, particularly for generating new data that includes inherent randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91fb93-d7da-49a2-8bda-c9d9b2757509",
   "metadata": {},
   "source": [
    "## Transition to Variational AE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb90b78-1249-453b-974b-152a245a2b17",
   "metadata": {},
   "source": [
    "#### 1. Encoder: Change to Probabilistic Encoding\n",
    "- Traditional AE: Encodes input into a fixed point in the latent space.\n",
    "\n",
    "- VAE Modification: Encodes input into a `mean` and `variance` for a probability distribution in the latent space, rather than a single point.\n",
    "\n",
    "- This change allows the model to sample from a range of values around the mean, capturing variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3990e357-06ab-4dba-afd3-9b7f052ed667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9818995c-8952-4e78-a512-7cd25ab69346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data representing height (cm) and weight (kg)\n",
    "data = np.array([\n",
    "    [160, 55],\n",
    "    [170, 65],\n",
    "    [180, 75],\n",
    "    [190, 85],\n",
    "    [200, 95]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41368a55-2b03-41f5-92b6-78e343b05d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>Weight (kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Height (cm)  Weight (kg)\n",
       "0          160           55\n",
       "1          170           65\n",
       "2          180           75\n",
       "3          190           85\n",
       "4          200           95"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a DataFrame for easy viewing\n",
    "df = pd.DataFrame(data, columns=['Height (cm)', 'Weight (kg)'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4091cde-7f91-4e48-9ff0-573335efef38",
   "metadata": {},
   "source": [
    "#### 2. Traditional AE Encoding vs. VAE Encoding\n",
    "\n",
    "- `Traditional AE Encoding`: Each input (height and weight) would map directly to a `single fixed point` in the latent space. So, our encoder might convert each height-weight pair into a unique 2D point like [1.5, -0.3], [0.8, 1.2], etc.\n",
    "\n",
    "- `VAE Encoding`: Instead of a single point, the encoder `maps each input` to a `mean` and `variance`. For instance, for an input [160, 55], the VAE might output a `mean` of [1.5, -0.3] and a `variance` of [0.2, 0.1].\n",
    "\n",
    "- This mean and variance define a normal distribution for each data point in the latent space.\n",
    "\n",
    "- In a VAE, we want to learn a distribution over the latent space rather than fixed deterministic points. By treating the output of z_mean as the center of a Gaussian distribution, we can encode each input as a distribution instead of a single point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340ef7e-d112-4555-b131-dc329fbcb4e7",
   "metadata": {},
   "source": [
    "- In a Variational Autoencoder (VAE), instead of encoding the input data into a single point, the encoder network learns to represent each input data point as a probability distribution. This distribution is defined by a mean and variance.\n",
    "\n",
    "    - `Mean`: The mean is the center of the distribution in the latent space. It is a `vector`, and each dimension of the vector corresponds to a dimension in the latent space.\n",
    "    - `Variance`: The variance defines the spread or uncertainty in the latent space for that particular point. Again, it's a `vector`, and each dimension corresponds to a variance for a particular latent dimension.\n",
    "      \n",
    "- **Interpretation from the VAE Objective Function**:\n",
    "  - The **KL divergence loss** in a VAE objective function compares the distribution represented by `z_mean` and `z_log_var` to a prior distribution (often a standard Gaussian).\n",
    "  - By minimizing this KL divergence, we ensure that the values in `z_mean` represent the central location of a Gaussian that resembles the prior, enforcing a **structured latent space**.\n",
    "  - Thus, although `z_mean` is computed via a dense layer, it acts as the **learned mean of the distribution** in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238dedf3-083e-4353-9f3c-868cdbda2d79",
   "metadata": {},
   "source": [
    "If we want to represent a simple `2D latent space`, we could have a `2D mean` and `2D variance`. \n",
    "\n",
    "Here's how that looks:\n",
    "\n",
    "- Input: [Height, Weight] = [160, 55]\n",
    "- Latent space (2D): The VAE would `learn` a `2D mean vector` and a `2D variance vector` that define a distribution over the 2D latent space.\n",
    "  \n",
    "Let’s say the VAE outputs the following for one data point:\n",
    "\n",
    "- Mean Latent Vector: [1.5, -0.3] (This could represent a point in the 2D latent space where the data is centered).\n",
    "- Variance Latent Vector: [0.2, 0.1] (This represents how much spread there is around the mean along each axis).\n",
    "- \n",
    "Now, the VAE does not simply map [160, 55] to the latent point [1.5, -0.3] directly. Instead, it creates a distribution centered around [1.5, -0.3] with a spread determined by [0.2, 0.1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac533cc6-e698-4cad-9d53-4efdab7ecd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset (Height, Weight) for 3 data points\n",
    "data_points = np.array([[160, 55], \n",
    "                        [165, 60], \n",
    "                        [170, 65]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f365a00b-823a-4456-8691-65e45e35cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder outputs (mean and variance for each data point)\n",
    "mean_latent     = np.array([[1.5, -0.3], \n",
    "                            [1.6, -0.2], \n",
    "                            [1.7, -0.1]])  # 3 data points, 2D means\n",
    "\n",
    "variance_latent = np.array([[0.2, 0.1], \n",
    "                            [0.3, 0.15], \n",
    "                            [0.25, 0.2]])  # 3 data points, 2D variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfe842-ddda-4cfe-936d-9b6c8d3506bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f48d3724-d529-41ee-bfdb-6f2eb0da3c2d",
   "metadata": {},
   "source": [
    "**Example** - encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450bc90-2b8b-4ebe-9165-e55db1967992",
   "metadata": {},
   "source": [
    "```python\n",
    "# a single data point, e.g., an image or a feature vector.\n",
    "encoder_input = Input(shape=(input_dim,), name='encoder_input')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac1f1c-7936-491d-9c6c-fe6a59830446",
   "metadata": {},
   "source": [
    "```python\n",
    "# fully connected Dense layer with 300 units and \n",
    "# a LeakyReLU activation function to learn features of the input data\n",
    "x = Dense(300)(encoder_input)\n",
    "x = LeakyReLU()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c306a8-44f0-4f7c-927f-ccd46ba24e3c",
   "metadata": {},
   "source": [
    "```python\n",
    "# the output of a Dense layer with latent_dim units.\n",
    "# Explanation: Each data point is mapped to a vector of size latent_dim \n",
    "# representing the mean of the latent distribution.  (Gaussian)\n",
    "# The output of this layer is the mean for the probabilistic latent space for each input data point.\n",
    "# For example, if latent_dim=2, then z_mean is a 2D vector: [mean_1, mean_2].\n",
    "# Center of the Gaussian in the latent space.\n",
    "z_mean    = Dense(latent_dim, name='z_mean')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73dbb8-7b42-457b-a79d-3123e10dd3d0",
   "metadata": {},
   "source": [
    "```python\n",
    "# the output of another Dense layer with latent_dim units.\n",
    "# does not require an explicit log() function\n",
    "# raw output of the Dense layer is treated as the log-variance\n",
    "# log transformation is implicitly applied in the sense that the Dense layer directly \n",
    "# predicts the logarithmic scale of the variance.\n",
    "# Later, during the reparameterization trick, the log-variance is exponentiated to \n",
    "# obtain the standard deviation, which is then used for sampling in the latent space.\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024e6f1-d3b6-4118-a691-3536e2f612bb",
   "metadata": {},
   "source": [
    "```python\n",
    "# Reparameterization Trick\n",
    "# Applies the reparameterization trick, enabling backpropagation through stochastic sampling.\n",
    "# Lambda(sampling): Custom layer that applies the sampling function to z_mean and z_log_var.\n",
    "# sampling function: Generates random latent points based on the mean and log-variance. \n",
    "# This is achieved by adding random noise to the mean, scaled by the standard deviation.\n",
    "z = Lambda(sampling, \n",
    "           output_shape=(latent_dim,), \n",
    "           name        ='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4431e-4b94-4bc1-8920-4fa735db839d",
   "metadata": {},
   "source": [
    "```python\n",
    "# # Build the encoder model\n",
    "encoder = Model(encoder_input, [z_mean, z_log_var, z], name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970098f-fa7d-468b-a175-a30287c0fda4",
   "metadata": {},
   "source": [
    "```python\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]  # Number of samples in the batch\n",
    "    dim   = K.int_shape(z_mean)[1]  # Dimensionality of the latent space\n",
    "    \n",
    "    # Sampling from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=(batch, dim))  # Standard normal noise\n",
    "\n",
    "    # Apply the reparameterization trick: z = mean + std * epsilon\n",
    "    z = z_mean + K.exp(0.5 * z_log_var) * epsilon  # Returns the latent variable z\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646d511-b9e2-403b-b575-33ec500b4ad2",
   "metadata": {},
   "source": [
    "**What happens during training?**\n",
    "\n",
    "- `Backpropagation`: The sampling function ensures that the sampling operation is differentiable, allowing gradients to flow through the reparameterization step and enabling the VAE to be trained with standard gradient descent.\n",
    "  \n",
    "- `During inference`: For inference (during testing or sampling), you can directly sample from the latent space using the learned mean and variance without needing to perform reparameterization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ac1e0-41e4-4e8a-b8ae-d3f990b36c37",
   "metadata": {},
   "source": [
    "#### Loss Function in VAE\n",
    "\n",
    "The **loss function** in a **Variational Autoencoder (VAE)** consists of two main components:\n",
    "1. **Reconstruction Loss**: Measures how well the decoder reconstructs the input data.\n",
    "2. **KL Divergence Loss**: Regularizes the latent space by measuring the difference between the learned latent space distribution and a prior (usually a standard normal distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe5c90-13e0-4b30-8555-1f62127bc270",
   "metadata": {},
   "source": [
    "#### 1. Reconstruction Loss\n",
    "\n",
    "The **reconstruction loss** is responsible for ensuring that the model can accurately reconstruct the input data. It measures how far the output (reconstructed data) is from the original input data.\n",
    "\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{Reconstruction Loss} = \\mathbb{E}_{q(z|x)}[ \\log p(x|z) ]\n",
    "  $$\n",
    "  Where:\n",
    "  - \\( q(z|x) \\) is the approximate posterior (encoded distribution) of the latent variables \\( z \\) given the input data \\( x \\).\n",
    "  - \\( p(x|z) \\) is the likelihood of the data \\( x \\) given the latent variable \\( z \\), which is modeled by the decoder.\n",
    "  \n",
    "  The most commonly used loss for reconstruction is either **Mean Squared Error (MSE)** or **Binary Cross-Entropy**, depending on the type of data being reconstructed. \n",
    "\n",
    "For example:\n",
    "- **MSE** is used for continuous data (like images).\n",
    "- **Binary Cross-Entropy** is used for binary data.\n",
    "\n",
    "In code:\n",
    "```python\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "# Assume x and x_reconstructed are the true and reconstructed inputs\n",
    "reconstruction_loss = MeanSquaredError()(x, x_reconstructed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d89cc-e1b7-4950-a1fe-dd126a41bd11",
   "metadata": {},
   "source": [
    "#### 2. KL Divergence Loss\n",
    "\n",
    "The KL Divergence Loss serves as a regularizer by ensuring that the learned latent space distribution $q(z \\mid x)$ is close to a known distribution (typically a standard normal distribution $\\mathcal{N}(0, I)$ ).\n",
    "- Formula:\n",
    "\n",
    "$$\n",
    "\\text { KL Divergence Loss }=D_{\\mathrm{KL}}(q(z \\mid x) \\| p(z))=\\frac{1}{2} \\sum_{i=1}^{D_z}\\left(\\exp \\left(\\log \\sigma_i^2\\right)+\\mu_i^2-1-\\log \\sigma_i^2\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $\\mu_i$ and $\\sigma_i^2$ are the mean and variance of the approximate posterior distribution $q(z \\mid x)$.\n",
    "- $p(z)$ is the prior distribution (usually $\\mathcal{N}(0, I)$ ).\n",
    "- $D_z$ is the dimensionality of the latent space."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0579ff32-fa31-4482-93d3-ef3c1e7f4ece",
   "metadata": {},
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def kl_divergence_loss(z_mean, z_log_var):\n",
    "    # Compute KL Divergence loss term\n",
    "    kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b64db82-2229-4106-81a5-b8287024cea5",
   "metadata": {},
   "source": [
    "# VAE loss\n",
    "reconstruction_loss = mse(encoder_input, outputs) * input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "vae.compile(optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
