{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea338e-05c4-4a56-8939-908189702ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# langchain_openai \n",
    "# langchain_community \n",
    "# langchain-text-splitters \n",
    "# langchain-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf18736-ba81-476f-b1d6-e195cc9bf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "22846aed-6e5b-4da1-a283-6b52f7cbec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini',\n",
    "                   #api_key= openai_api_key,\n",
    "                  max_tokens= 100)\n",
    "\n",
    "#model = ChatOpenAI(model='gpt-3.5-turbo-instruct-0914')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bdb754a1-5e80-47f1-bcb1-58f6d92304af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The sky is'\n",
    "prompt = '''\n",
    "\n",
    "The sky is . \n",
    "\n",
    "Provide 5 top suitable color options\n",
    "\n",
    "Example\n",
    "the color of chair is orange\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e2c81383-1f9c-4b6b-bbbc-f7c0d4d58501",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "78bc9de7-6899-4337-a0d0-34cff9578a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. The sky is blue.\\n2. The sky is gray.\\n3. The sky is pink.\\n4. The sky is purple.\\n5. The sky is orange.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f6dc4b-953d-4593-9a76-adb8d04d1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are five suitable color options for \"the sky is\":\\n\\n1. The sky is blue.\\n2. The sky is gray.\\n3. The sky is pink.\\n4. The sky is golden.\\n5. The sky is lavender.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 30, 'total_tokens': 77, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_9b78b61c52', 'finish_reason': 'stop', 'logprobs': None}, id='run-9a5af2b0-24dc-466a-acd8-def2c928b9c8-0', usage_metadata={'input_tokens': 30, 'output_tokens': 47, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef28e5c-0c1c-4309-a609-a5f4cdb05737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Here are five suitable color options for \"the sky is\":\\n\\n1. The sky is blue.\\n2. The sky is gray.\\n3. The sky is pink.\\n4. The sky is golden.\\n5. The sky is lavender.',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 47,\n",
       "   'prompt_tokens': 30,\n",
       "   'total_tokens': 77,\n",
       "   'completion_tokens_details': {'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'accepted_prediction_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "  'system_fingerprint': 'fp_9b78b61c52',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-9a5af2b0-24dc-466a-acd8-def2c928b9c8-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 30,\n",
       "  'output_tokens': 47,\n",
       "  'total_tokens': 77,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f4740c7-d903-464d-96b8-2c76e5147f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05f57446-1bf1-423b-814c-2d146bed6b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_kwargs': {'refusal': None},\n",
      " 'content': 'Here are five suitable color options for \"the sky is\":\\n'\n",
      "            '\\n'\n",
      "            '1. The sky is blue.\\n'\n",
      "            '2. The sky is gray.\\n'\n",
      "            '3. The sky is pink.\\n'\n",
      "            '4. The sky is golden.\\n'\n",
      "            '5. The sky is lavender.',\n",
      " 'example': False,\n",
      " 'id': 'run-9a5af2b0-24dc-466a-acd8-def2c928b9c8-0',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'finish_reason': 'stop',\n",
      "                       'logprobs': None,\n",
      "                       'model_name': 'gpt-4o-mini-2024-07-18',\n",
      "                       'system_fingerprint': 'fp_9b78b61c52',\n",
      "                       'token_usage': {'completion_tokens': 47,\n",
      "                                       'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                                                     'audio_tokens': 0,\n",
      "                                                                     'reasoning_tokens': 0,\n",
      "                                                                     'rejected_prediction_tokens': 0},\n",
      "                                       'prompt_tokens': 30,\n",
      "                                       'prompt_tokens_details': {'audio_tokens': 0,\n",
      "                                                                 'cached_tokens': 0},\n",
      "                                       'total_tokens': 77}},\n",
      " 'tool_calls': [],\n",
      " 'type': 'ai',\n",
      " 'usage_metadata': {'input_token_details': {'audio': 0, 'cache_read': 0},\n",
      "                    'input_tokens': 30,\n",
      "                    'output_token_details': {'audio': 0, 'reasoning': 0},\n",
      "                    'output_tokens': 47,\n",
      "                    'total_tokens': 77}}\n"
     ]
    }
   ],
   "source": [
    "# Pretty-print the AI message object\n",
    "pprint(dict(completion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20644a8-7930-4a1f-b997-efbb18e27ba4",
   "metadata": {},
   "source": [
    "#### Example 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22997d8a-7e5f-46bc-83a2-11295dcfcff8",
   "metadata": {},
   "source": [
    "#### System role\n",
    "Enables the developer to specify instructions the model should use to answer a user question.\n",
    "\n",
    "#### User role\n",
    "The individual asking questions and generating the queries sent to the model.\n",
    "\n",
    "#### Assistant role\n",
    "The model’s responses to the user’s query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e5b70-3fba-4523-b1f7-07481d0da11b",
   "metadata": {},
   "source": [
    "langchain way ...\n",
    "\n",
    "#### HumanMessage\n",
    "A message sent from the perspective of the human (user), with the user role.\n",
    "\n",
    "#### AIMessage\n",
    "A message sent from the perspective of the AI the human is interacting with, with the assistant role.\n",
    "\n",
    "#### SystemMessage\n",
    "A message setting the instructions the AI should follow, with the system role.\n",
    "\n",
    "#### ChatMessage\n",
    "A message allowing for arbitrary setting of roles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ef3e85ca-db7f-4a0d-9825-136bf93428e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1003833e-1f69-4ff8-a063-ffac46c890d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f826cbe9-dc9f-4956-afd4-999c2691daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "08a486d7-284e-4fc4-ba6b-c2bca0893129",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [HumanMessage('What is the capital of France')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d1aaf38f-b707-40a2-9e3a-5780333b4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "79f87fac-05d6-42d0-adc2-888114b583a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 13, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4a2c6e6d-357a-4336-ac59-15d592bfaf47-0', usage_metadata={'input_tokens': 13, 'output_tokens': 7, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "82d2d238-f08d-45fe-b39b-6da350132bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "250dead9-cb73-4528-8f33-e16d36a17665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "82829000-98b9-4325-8ace-bcbdf75be17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b046c498-04a9-4197-95b2-55ab90e7c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage('You are a helpful assistant')\n",
    "human_msg  = HumanMessage('What is the capital of India')\n",
    "\n",
    "completion = model.invoke([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c9131f-f95d-46f2-99bd-8054e8e2b2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is New Delhi.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 22, 'total_tokens': 30, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-54ec4969-a91a-4ac9-81ce-05216a87e295-0', usage_metadata={'input_tokens': 22, 'output_tokens': 8, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729b079-245d-461c-abb1-38b1d21e3ec8",
   "metadata": {},
   "source": [
    "#### Making LLM prompts reusable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af1e80a9-2321-442b-a9ac-95fa2d801f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19db57bd-4ed5-4218-a8c4-31490bdced0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reusable prompt template\n",
    "template = \"What is the capital of {country}?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38bd5187-e466-45cd-bd93-5dc1a1796645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PromptTemplate object with a placeholder for 'country'\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= [\"country\"],\n",
    "    template       = template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7b02ac7-e92d-4860-97e2-45fad511b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI model\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a40530c4-b385-45b0-9251-c216e49e4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the template to fill in different countries\n",
    "query_france  = prompt.format(country=\"France\")\n",
    "query_germany = prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95ab0641-5ea3-4c70-af9d-0baf5f0d75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "response_france  = llm.invoke(query_france)\n",
    "response_germany = llm.invoke(query_germany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99571ff1-3bea-4e70-a165-1438ca9e71d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "\n",
      "The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "print(response_france)\n",
    "print(response_germany)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dd062-58fb-4be1-a354-1181a3e19f5b",
   "metadata": {},
   "source": [
    "#### Example 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fb3bfa96-2a7f-4e49-981b-f61f8bb26a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c845325e-3cf2-49d5-bd77-1125356c1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ad83ad6e-2656-4614-9df4-c82f8f6b36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm.chat.completions.create(\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"user\", \"content\": \"what is Data Science\"}\n",
    "#                 ],\n",
    "#                 model=\"gpt-4o-mini\",\n",
    "#                 max_tokens = 10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc51ef-6c34-4e05-9a25-777b36026989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "726c18fc-ffaa-4375-8f89-77d4aaacaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reusable HumanMessage with placeholders for 'country' and 'capital'\n",
    "def create_human_message(country, capital):\n",
    "    return HumanMessage(content=f\"Desribe {capital} of the {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9b49fa7a-b3ff-46f6-bbb2-f82fec2470a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HumanMessage for different countries and capitals\n",
    "human_message_france  = create_human_message(country=\"France\", capital=\"Paris\")\n",
    "human_message_germany = create_human_message(country=\"Germany\", capital=\"Berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e111ae79-7f06-41e7-a382-158a242dceb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Invoke the model with these human messages\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response_france  \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m([human_message_france])\n\u001b[0;32m      3\u001b[0m response_germany \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke([human_message_germany])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OpenAI' object has no attribute 'invoke'"
     ]
    }
   ],
   "source": [
    "# Invoke the model with these human messages\n",
    "response_france  = llm.invoke([human_message_france])\n",
    "response_germany = llm.invoke([human_message_germany])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "96f1a2c9-6f67-4c33-b781-dc0b313db1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Paris, the capital city of France, is known for its romantic charm, rich history, and cultural heritage. It is situated on the banks of the river Seine, in the northern central part of the country. The city is divided into 20 arrondissements, each with its own distinct character and attractions.\n",
      "\n",
      "The most iconic landmark of Paris is the Eiffel Tower, a towering structure that offers breathtaking views of the city. Other notable attractions include the Notre-Dame Cathedral, the Louvre Museum, and the Arc de Triomphe.\n",
      "\n",
      "Paris is also known for its picturesque streets lined with charming cafes, boutique shops, and beautiful architecture. The city is a hub of art, fashion, and cuisine, with world-renowned museums, designer stores, and Michelin-starred restaurants.\n",
      "\n",
      "The city is also home to several green spaces, including the Luxembourg Gardens and the Tuileries Garden, providing peaceful oases in the midst of the bustling city.\n",
      "\n",
      "Parisians are known for their chic style and love for leisurely activities such as picnics, strolls along the river, and enjoying a glass of wine at a sidewalk cafe. The city comes alive at night, with its vibrant nightlife scene, including lively bars, clubs, and cabarets.\n",
      "\n",
      "Overall\n"
     ]
    }
   ],
   "source": [
    "print(response_france)  # Output: The capital of France is Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fb23017-59da-4c6f-a2a3-64281545af87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Berlin, the capital city of Germany, is a vibrant and diverse metropolis known for its rich history, cultural landmarks, and cosmopolitan atmosphere. Situated in the northeastern part of the country, Berlin is a bustling city with a population of over 3.5 million people.\n",
      "\n",
      "At the heart of Berlin lies the iconic Brandenburg Gate, a symbol of the city's tumultuous past and its reunification. This monument is surrounded by the wide boulevard Unter den Linden, lined with historical buildings, trendy cafes, and luxury shops. Nearby, the majestic Reichstag building, with its glass dome, is a must-visit for its impressive architecture and panoramic views of the city.\n",
      "\n",
      "One of the most striking features of Berlin is its unique mix of old and new. The city is dotted with remnants of its dark history, including the Berlin Wall, Checkpoint Charlie, and the Holocaust Memorial, which serve as reminders of the city's past struggles. At the same time, Berlin is a hub of modern culture, with a thriving art scene, innovative architecture, and a vibrant nightlife.\n",
      "\n",
      "The city also boasts a plethora of world-class museums, such as the Pergamon Museum, which houses ancient artifacts, and the East Side Gallery, an open-air exhibition of murals painted on\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(response_germany)  # Output: The capital of Germany is Berlin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a9030-063f-4422-bcdc-0cede6ae9991",
   "metadata": {},
   "source": [
    "#### Example - 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "491c1196-9473-42d6-8b25-b187bd8bf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "359d1d76-498b-461d-8a43-971a14b44eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        Answer the question based on the context given as input\n",
    "        \n",
    "             Context:  {context} \n",
    "             Question: {question} \n",
    "             Answer: \n",
    "             \n",
    "             \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae60b016-2595-48f9-ba8e-2201f3028870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adb39165-b983-4780-909b-66eaddd0e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.invoke({ \n",
    "    \"context\": '''advancements in Natural Language Processing (NLP) are prominently marked by \n",
    "    enhanced language models that deliver more coherent and contextually relevant text generation. \n",
    "    ''' ,\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    " }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "327b88f2-eee8-469a-84fe-912312c054e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='\\n        Answer the question based on the context given as input\\n        \\n             Context:  advancements in Natural Language Processing (NLP) are prominently marked by \\n    enhanced language models that deliver more coherent and contextually relevant text generation. \\n     \\n             Question: Which model providers offer LLMs? \\n             Answer: \\n             \\n             '\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bc37495-4420-4697-a667-81aded87d13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            Some examples of model providers that offer LLMs are Google, OpenAI, and BERT.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02109b28-b0cd-416e-857a-daca407a0cfc",
   "metadata": {},
   "source": [
    "Another way ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d24ecb2-e1c8-40cf-bae2-ac250d32e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template with placeholders\n",
    "template = PromptTemplate(\n",
    "    input_variables= [\"context\", \"question\"],\n",
    "    template       = \"Context: {context}\\nQuestion: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4239e14-8925-4ccc-b359-627672a6e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context and question\n",
    "context = \"The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\"\n",
    "question = \"Which model providers offer LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "568d4678-4c24-45e7-9b43-3ea433cef22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the template with the provided context and question\n",
    "formatted_prompt = template.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": question\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b587b11-e442-4b48-89e5-02d48654d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI model\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f09f9dd-86dd-4c2f-a57a-f08dea40a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the formatted prompt\n",
    "response = llm.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "198562c9-3c6b-4045-b0b5-070052d76156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "text='Context: The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\\nQuestion: Which model providers offer LLMs?'\n",
      "\n",
      "Response:\n",
      "\n",
      "\n",
      "Some of the model providers that offer LLMs (Language Model Machines) include Google, Microsoft, OpenAI, and Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "# Print the formatted prompt and response\n",
    "print(\"Formatted Prompt:\")\n",
    "print(formatted_prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753da675-6223-4122-96f8-4225f73ca568",
   "metadata": {},
   "source": [
    "#### chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e1460561-3b24-45a6-8ac4-958476a70d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c19b0ac-02ce-45f4-b6c1-351adbb78147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chat prompt template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer the question based on the provided context.'),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "57600eef-8581-4970-a4ed-cdf2cd424abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context and question\n",
    "context  = \"The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\"\n",
    "question = \"Which model providers offer LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bbbb868e-1d7b-4402-975d-eaa9a91ac453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the template with the provided context and question\n",
    "formatted_prompt = template.invoke({\n",
    "    \"context\":  context,\n",
    "    \"question\": question\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d01d9da4-9362-4d9d-9833-146164a3bf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Answer the question based on the provided context.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Context: The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3a68bc85-a9c4-4cc3-abf5-0e4f0eaa39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming formatted_prompt is your ChatPromptTemplate instance with messages\n",
    "messages_content = \"\\n\".join([msg.content for msg in formatted_prompt.messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8eee6f43-b5d9-4f64-b535-ac42199c35cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the provided context.\n",
      "Context: The most recent advancements in Natural Language Processing include improvements in transformer architectures, fine-tuning techniques, and the development of more efficient models.\n",
      "Question: Which model providers offer LLMs?\n"
     ]
    }
   ],
   "source": [
    "print(messages_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b19f6d78-3d07-4f64-9271-3e801930f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI model\n",
    "llm = OpenAI(model = 'gpt-3.5-turbo',\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "80b19d20-7298-4af8-871a-4dcddcfae60a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error code: 403 - {'error': {'message': 'Project `proj_23vI14HKrTzW6pFJIQtRQkCB` does not have access to model `gpt-3.5-turbo`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate a response using the messages content\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:390\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    388\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    402\u001b[0m     )\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:755\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    749\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    754\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m         )\n\u001b[0;32m    949\u001b[0m     ]\n\u001b[1;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    791\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    793\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:779\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    771\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    776\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 779\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    783\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    788\u001b[0m         )\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\langchain_community\\llms\\openai.py:463\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    452\u001b[0m         {\n\u001b[0;32m    453\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m         }\n\u001b[0;32m    461\u001b[0m     )\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\langchain_community\\llms\\openai.py:121\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\openai\\resources\\completions.py:539\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    537\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    538\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\openai\\_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1276\u001b[0m     )\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\openai\\_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ANACONDA3\\envs\\langchain_env\\Lib\\site-packages\\openai\\_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1067\u001b[0m )\n",
      "\u001b[1;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'error': {'message': 'Project `proj_23vI14HKrTzW6pFJIQtRQkCB` does not have access to model `gpt-3.5-turbo`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "# Generate a response using the messages content\n",
    "response = llm.invoke([SystemMessage(content=messages_content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bebde6-8ccd-4e11-8ac4-9d27d0bb3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the formatted prompt and response\n",
    "print(\"Formatted Prompt:\")\n",
    "print(messages_content)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a870d9-04d0-4474-a6a8-9c563bc92ff8",
   "metadata": {},
   "source": [
    "| **Feature**              | **PromptTemplate**                                         | **ChatPromptTemplate**                                      |\n",
    "|--------------------------|------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Use Case**             | Single-turn or one-shot tasks (e.g., questions, completions).| Multi-turn interactions (e.g., conversations, chat history). |\n",
    "| **Structure**            | Single text with placeholders (e.g., `\"{variable}\"`).       | Multiple structured messages (e.g., `SystemMessage`, `HumanMessage`). |\n",
    "| **Message Types**        | Only one generic type of input (text).                     | Supports different message roles: `SystemMessage`, `HumanMessage`, `AIMessage`. |\n",
    "| **Conversation Context** | No inherent context management across interactions.        | Manages multi-turn interactions with contextual messages.    |\n",
    "| **Flexibility**          | Simple, best for straightforward formatting of inputs.     | More flexible for handling complex interactions and roles.   |\n",
    "| **Typical Application**  | Basic tasks (e.g., generating a single prompt or response). | Chatbots, dialogue systems, or any AI that manages conversations. |\n",
    "| **Example**              | `The capital of {country} is {capital}.`                   | `System: \"You are an assistant.\"` <br> `Human: \"What is the capital of {country}?\"` |\n",
    "| **Format**               | Direct text formatting.                                    | Multi-role chat with message objects for system, human, and AI roles. |\n",
    "| **When to Use**          | For simple tasks where single-shot prompting is enough.    | For scenarios that require maintaining chat history or multiple roles. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9617dd77-cd97-4770-948f-5a39c52a8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reusable ChatPromptTemplate with system and human messages\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer the question based on the provided context.'),\n",
    "    ('human',  'Context: {context}'),\n",
    "    ('human',  'Question: {question}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fe1b5ec0-1d1d-473a-adee-e44a30ae8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ChatOpenAI model\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eec5a4b7-5158-498e-8cf4-522aceea4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the template with specific context and question\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"The most recent advancements in NLP and AI.\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad93d4bd-41cf-4b14-b1bb-f771bc239529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the completion from the model\n",
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43e6c919-f37d-4220-bcde-2dd732cfa5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some of the model providers that offer Large Language Models (LLMs) include OpenAI with their GPT series (such as GPT-3), Google with BERT (Bidirectional Encoder Representations from Transformers), and Microsoft with Turing-NLG.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72e76d-e297-46ee-9818-6526e87b3272",
   "metadata": {},
   "source": [
    "#### Getting Specific Formats out of  LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929529e-e3e2-4ec1-8dbb-d6bcba80b871",
   "metadata": {},
   "source": [
    "When generating JSON, the first task is to define the schema you want the LLM to respect when producing the output. \n",
    "\n",
    "Then, you should include that schema in the prompt, along with the text you want to use as the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76f27628-c6b0-4f09-9f31-0b9fc1a10947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel  # Importing directly from pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fbf5cbeb-b1e3-47b6-b61c-0e9722dc4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom structure for the answer and justification\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification.'''\n",
    "    answer:        str  # The answer to the user's question\n",
    "    justification: str  # Justification for the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb9d62d8-4d0e-4ff9-99ba-af979afc7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ChatOpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1c947ee8-be21-44e8-b0c1-4351d5d9e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output by using the defined BaseModel\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90e22c64-ca7d-40bf-a93e-1044c3a90010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a query\n",
    "result = structured_llm.invoke(\"What weighs more, a pound of feathers or a pound of bricks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd347e02-e9a7-44a4-bcdd-cff0b361c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of feathers and a pound of bricks weigh one pound. The difference lies in the volume and density of the materials.')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "705ca74f-f2a2-4d70-8fd7-8a1c64f2cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='They weigh the same.' justification='Both a pound of feathers and a pound of bricks weigh one pound. The difference lies in the volume and density of the materials.'\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab80ca5f-e079-4d7d-8337-549c097f65a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'They weigh the same.',\n",
       " 'justification': 'Both a pound of feathers and a pound of bricks weigh one pound. The difference lies in the volume and density of the materials.'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21036ced-bd55-48a4-801e-a3f1bc6711e3",
   "metadata": {},
   "source": [
    "**Example 1:** Returning Movie Recommendations with Justifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3feb3837-df27-4e6b-b129-9de4a9297779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for movie recommendations\n",
    "class MovieRecommendation(BaseModel):\n",
    "    movie_title:   str  # The title of the recommended movie\n",
    "    genre:         str  # The genre of the movie\n",
    "    justification: str  # Why this movie was recommended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af009668-1589-4d0c-b1f9-ffde5b799ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c7b1613d-32df-43df-ae8a-dbdfb0942fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(MovieRecommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "637a68c2-484c-4062-8a15-e09923f2f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke with a prompt asking for movie recommendations\n",
    "prompt = \"Can you recommend a sci-fi movie and explain why it’s a good choice?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c9e1b24-fd19-49c8-9d19-fd5a1867a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a5651ef1-17c9-4dfd-9bd0-e302b961910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_title='Interstellar' genre='Sci-Fi' justification=\"Interstellar is a highly acclaimed movie directed by Christopher Nolan. It's a great choice for those who love sci-fi movies because it combines mind-bending concepts about space and time with a compelling human story. The film is visually stunning, and it's backed by solid performances from a strong cast, including Matthew McConaughey and Anne Hathaway. Moreover, it tackles complex ideas like black holes, time dilation, and wormholes in a way that's accessible to a general audience. The film also boasts an emotional core, exploring themes of love, sacrifice, and the survival of the human race.\"\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f65fc26-04fd-4b77-9eef-0e0d4b4db948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_title': 'Interstellar',\n",
       " 'genre': 'Sci-Fi',\n",
       " 'justification': \"Interstellar is a highly acclaimed movie directed by Christopher Nolan. It's a great choice for those who love sci-fi movies because it combines mind-bending concepts about space and time with a compelling human story. The film is visually stunning, and it's backed by solid performances from a strong cast, including Matthew McConaughey and Anne Hathaway. Moreover, it tackles complex ideas like black holes, time dilation, and wormholes in a way that's accessible to a general audience. The film also boasts an emotional core, exploring themes of love, sacrifice, and the survival of the human race.\"}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad752f-c30b-4335-a91d-30d2cd608874",
   "metadata": {},
   "source": [
    "**Example 2:** Returning Product Reviews with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a49feae1-fe3b-4382-a2da-74ab083269f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for product review\n",
    "class ProductReview(BaseModel):\n",
    "    product_name: str    # Name of the product being reviewed\n",
    "    score:        float  # Rating out of 5\n",
    "    review:       str    # Review explanation for the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7ca33cc-5ccd-4240-a50d-d63108c89553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de6c9d91-e884-47b7-b7bd-aad8a701c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(ProductReview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56d92192-0346-47cf-9950-35410845a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a prompt about product reviews\n",
    "prompt = \"Can you review the iPhone 14 and give it a score out of 5?\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01e7fba6-b04a-45e4-9641-fafed10ca613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_name': 'iPhone 14',\n",
       " 'score': 4.0,\n",
       " 'review': 'The iPhone 14 is a great device with impressive performance and features. It has a sleek design, powerful hardware, and a fantastic camera. The only downside is the high price tag.'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac73f52-87a8-4065-818e-7dab8a5f6120",
   "metadata": {},
   "source": [
    "**Example 3:** Returning Travel Itinerary with Activity Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4edc9df1-c018-40c3-9731-682757b4b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for travel itinerary\n",
    "class TravelItinerary(BaseModel):\n",
    "    destination: str        # Destination city or country\n",
    "    days:        int        # Number of days for the itinerary\n",
    "    activities:  list[str]  # List of suggested activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "62e985c3-e4b7-441d-ab8e-bccb44114859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ea835a61-23e2-438e-8d28-fd128148f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(TravelItinerary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "58f80dba-eb38-4e98-838a-4e548b752719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a travel prompt\n",
    "prompt = \"Plan a 3-day trip to Paris with activity suggestions for each day.\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "99387236-b33a-4d8e-b905-fbe0cfe6ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "destination='Paris' days=3 activities=['Visit the Eiffel Tower', 'Explore the Louvre Museum', 'Stroll through Montmartre', 'Relax at Luxembourg Gardens', 'Take a Seine River cruise', 'Visit Notre-Dame Cathedral', 'Discover the Palace of Versailles']\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0fb7252-98cd-4604-bcbb-ff08bb3a6850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'destination': 'Paris',\n",
       " 'days': 3,\n",
       " 'activities': ['Visit the Eiffel Tower',\n",
       "  'Explore the Louvre Museum',\n",
       "  'Stroll through Montmartre',\n",
       "  'Relax at Luxembourg Gardens',\n",
       "  'Take a Seine River cruise',\n",
       "  'Visit Notre-Dame Cathedral',\n",
       "  'Discover the Palace of Versailles']}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8b371-c72a-4c64-ac00-492b906b131c",
   "metadata": {},
   "source": [
    "**Example 4:** Structured Weather Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "254fb93e-11d7-4a0b-8283-b6b31fe76fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for weather forecast\n",
    "class WeatherForecast(BaseModel):\n",
    "    city:        str  # Name of the city\n",
    "    date:        str  # Forecast date\n",
    "    temperature: str  # Temperature in Celsius/Fahrenheit\n",
    "    description: str  # Short weather description (e.g., sunny, cloudy, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6df8822-dd4c-43d9-9715-78cc0209911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5cd153b-334f-4377-815d-880062b6b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(WeatherForecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "71443faf-0f94-4cbe-9e7b-444d1c02b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a weather forecast prompt\n",
    "prompt = \"What will the weather be like in New York tomorrow?\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "50b717e0-b01f-4460-acd1-4447be2f4390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'New York',\n",
       " 'date': 'tomorrow',\n",
       " 'temperature': '75°F',\n",
       " 'description': 'sunny'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b71683-ce30-4ab4-838b-788468dcbdff",
   "metadata": {},
   "source": [
    "**Example 5:** Structured Customer Feedback Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0be57074-2546-4d4b-9529-1cc79922a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for customer feedback\n",
    "class CustomerFeedback(BaseModel):\n",
    "    feedback_summary:  str  # A summary of the feedback\n",
    "    overall_sentiment: str  # Overall sentiment (Positive, Neutral, Negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "759a57a0-273c-4276-b17b-a599bc8d569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "84e137b8-f68d-4c2b-a690-2300d7ab4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output\n",
    "structured_llm = llm.with_structured_output(CustomerFeedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d1f73f8-3354-4b9e-a1dd-310c62ccea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a customer feedback prompt\n",
    "prompt = \"Summarize the customer feedback for our product launch.\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dc36e0e8-8921-4309-9706-bf330790e106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feedback_summary': \"The product launch received a mix of positive and negative feedback. Many customers praised the innovative features and user-friendly design, highlighting the product's effectiveness in solving their problems. However, some users reported issues with durability and functionality, expressing disappointment with the initial performance. Overall, customers appreciated the concept but suggested improvements for future iterations.\",\n",
       " 'overall_sentiment': 'mixed'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39983af1-142d-4bde-9713-826013958301",
   "metadata": {},
   "source": [
    "**Example 5:** Structured Customer Feedback Summary with Sentiment and Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "297f3f8c-6261-4ad7-9411-9ba8a216dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "983e90ed-0112-4468-adac-02683b234f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structure for customer feedback with sentiment and confidence score\n",
    "class CustomerFeedback(BaseModel):\n",
    "    feedback_summary:  str  # A summary of the feedback\n",
    "    overall_sentiment: str   = Field(..., pattern =\"^(Positive|Negative|Neutral)$\")  # Ensures the sentiment is one of these values\n",
    "    score:             float = Field(..., gt=0, le=10)                            # Confidence score (float, 1 to 10 scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "16dcce32-495c-4e9c-9c8d-b186f66b1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bf14f734-6ebc-4022-84de-29db670eb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable structured output with the new model\n",
    "structured_llm = llm.with_structured_output(CustomerFeedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5ac2748c-7770-49e3-afb7-2171307424f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with a customer feedback prompt\n",
    "prompt = \"Summarize the customer feedback for our product launch and provide a sentiment with a confidence score.\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8720fa53-e5b2-4e11-9591-9d6ad40af7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feedback_summary': 'The product launch was well-received, with many customers praising its innovative features and user-friendly design. However, some users reported issues with the initial setup and customer service response times. Overall, the feedback indicates a strong interest in the product, but there are areas for improvement.',\n",
       " 'overall_sentiment': 'Positive',\n",
       " 'score': 8.5}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "55d4c42d-7e23-4180-b5a2-576c025861db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3010652f-5cf6-4f62-826a-1651d9188746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feedback_summary': 'The product launch was well-received, with many '\n",
      "                     'customers praising its innovative features and '\n",
      "                     'user-friendly design. However, some users reported '\n",
      "                     'issues with the initial setup and customer service '\n",
      "                     'response times. Overall, the feedback indicates a strong '\n",
      "                     'interest in the product, but there are areas for '\n",
      "                     'improvement.',\n",
      " 'overall_sentiment': 'Positive',\n",
      " 'score': 8.5}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2f1e1-b21f-4ca5-b8c9-65a0560f8ce1",
   "metadata": {},
   "source": [
    "... providing review as input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "355716bd-251b-4ab5-8912-91bf8cc6cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample customer feedback text\n",
    "feedback_text = \"\"\"\n",
    "I recently purchased your new gadget and I couldn't be happier! \n",
    "The performance exceeded my expectations, and the design is sleek. \n",
    "However, I think the battery life could be improved. \n",
    "Overall, I'm very satisfied with my purchase.\n",
    "\"\"\"\n",
    "\n",
    "feedback_text = \"\"\"\n",
    "Earth is flat\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "620e3865-7c70-43c3-8dfd-8357cea920e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for summarization and sentiment classification\n",
    "prompt = f\"Summarize the following customer feedback and provide a sentiment with a confidence score:\\n\\n{feedback_text}\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "95d85033-499c-4b4c-9a21-19484a12e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feedback_summary': 'The customer believes that the Earth is flat.',\n",
      " 'overall_sentiment': 'Negative',\n",
      " 'score': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Pretty print the result\n",
    "pprint(result.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c5a89-f31a-4d2d-8ced-d41bc174fc5e",
   "metadata": {},
   "source": [
    "Further normalization of the sentiment class ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "67f6ed04-bf47-4b1d-ae5c-86eb296e46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import  validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9c6f3ad4-714f-4d04-85bf-53f2ec20ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhupe\\AppData\\Local\\Temp\\ipykernel_8424\\4142650417.py:6: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  @validator('overall_sentiment', pre=True)\n"
     ]
    }
   ],
   "source": [
    "class CustomerFeedback(BaseModel):\n",
    "    feedback_summary: str\n",
    "    overall_sentiment: str = Field(..., pattern=\"^(Positive|Negative|Neutral)$\")\n",
    "    score: float = Field(..., gt=0, le=10)\n",
    "\n",
    "    @validator('overall_sentiment', pre=True)\n",
    "    def normalize_sentiment(cls, value):\n",
    "        if isinstance(value, str):\n",
    "            value_lower = value.lower()\n",
    "            if \"positive\" in value_lower:\n",
    "                return \"Positive\"\n",
    "            elif \"negative\" in value_lower:\n",
    "                return \"Negative\"\n",
    "            else:\n",
    "                return \"Neutral\"\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bd9ee499-b1f8-4490-951f-fcc7b0464d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample LLM setup and invocation\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "structured_llm = llm.with_structured_output(CustomerFeedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fa0e6226-fac3-42cc-9d06-a6070ee1f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example feedback text\n",
    "feedback_text = \"I feel it was a mildly positive experience.\"\n",
    "prompt = f\"Summarize the following customer feedback and provide a sentiment with a confidence score:\\n\\n{feedback_text}\"\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f1f3ab35-8aac-40ad-bc3e-35f2bd6be5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CustomerFeedback instance to enforce validation and normalization\n",
    "feedback_instance = CustomerFeedback(**result.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4ca56e67-a975-49bd-9d7f-87164861b190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerFeedback(feedback_summary='Mildly positive experience', overall_sentiment='Positive', score=7.0)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "87f1dc0b-88ce-4c44-ae06-f096605435f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feedback_summary': 'Mildly positive experience',\n",
      " 'overall_sentiment': 'Positive',\n",
      " 'score': 7.0}\n"
     ]
    }
   ],
   "source": [
    "pprint(feedback_instance.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60c876-3372-4252-8ff5-03f4eacd0a22",
   "metadata": {},
   "source": [
    "#### getting confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9d7550bf-32ff-4eb3-abe0-354576303f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6bde2f05-82d2-44ed-abf9-d14f310f7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pydantic model for the output\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    sentiment: str   = Field(..., pattern=\"^(Positive|Negative|Neutral)$\")\n",
    "    score:     float = Field(..., gt=0, le=10, description=\"Confidence score on a scale from 1 to 10.\")\n",
    "\n",
    "    @field_validator('sentiment', mode='before')\n",
    "    def normalize_sentiment(cls, value):\n",
    "        # Normalize different sentiment outputs\n",
    "        if isinstance(value, str):\n",
    "            value_lower = value.lower()\n",
    "            if \"positive\" in value_lower:\n",
    "                return \"Positive\"\n",
    "            elif \"negative\" in value_lower:\n",
    "                return \"Negative\"\n",
    "            else:\n",
    "                return \"Neutral\"\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "feabc633-80c8-42e1-ba7c-ffb48444a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4af8e7db-48a6-439d-8841-5bd3f8d64693",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(SentimentAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5955d633-19a4-4424-8d9c-6242b573013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example customer feedback text\n",
    "feedback_text = \"The service was excellent, but there was a slight delay.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b35c2cfc-8946-48b5-bf96-4b14ab9f27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed prompt asking for sentiment and confidence score\n",
    "\n",
    "prompt = (\n",
    "    f\"Analyze the following feedback and provide the sentiment classification \"\n",
    "    f\"('Positive', 'Negative', 'Neutral') along with a confidence score between (1 to 10):\\n\\n\"\n",
    "    f\"{feedback_text}\"\n",
    ")\n",
    "\n",
    "# prompt = (\n",
    "#     f\"Analyze the following feedback and provide the sentiment classification \"\n",
    "#     f\"('Positive', 'Negative', 'Neutral') along with a confidence score between (10 to 100):\\n\\n\"\n",
    "#     f\"{feedback_text}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b471f718-3884-4438-8a3e-6b5e570708a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model\n",
    "result = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a9480256-601e-4d60-bee8-9166355e7a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 5.0, 'sentiment': 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "# Pretty print the result\n",
    "pprint(result.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73d777-f8b0-4af9-83d7-1b35291f76d5",
   "metadata": {},
   "source": [
    "#### Other Machine-Readable Formats with  Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd00f3c-e73d-461a-bcf7-556a7012bb16",
   "metadata": {},
   "source": [
    "##### Using LLMs for Structured Outputs\n",
    "\n",
    "You can leverage a large language model (LLM) or chat model to generate outputs in various formats, including CSV or XML. This is where output parsers become beneficial. \n",
    "\n",
    "##### Functions of Output Parsers\n",
    "\n",
    "Output parsers are classes designed to help structure responses from large language models. They serve two primary purposes:\n",
    "\n",
    "1. **Providing Format Instructions**  \n",
    "   Output parsers can incorporate additional guidelines in the prompt, assisting the LLM in generating text in a specific format that the parser can easily interpret.\n",
    "\n",
    "2. **Validating and Parsing Output**  \n",
    "   The main role of output parsers is to take the textual output from the LLM or chat model and convert it into a more structured format, such as a list or XML. This process may involve eliminating unnecessary information, correcting incomplete output, and validating the parsed values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7dcc7eda-e435-4d49-9828-6b6e523f1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b49d160d-4544-44e5-a976-48fb61cfd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the output parser\n",
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a45f50b8-43b3-4f45-94bd-3c774f27586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input string\n",
    "input_string = \"apple, banana, cherry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "18e23b2b-5d79-41ae-a94b-e0c1779de360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the input string using the output parser\n",
    "items = parser.invoke(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "87b30057-ce3b-4436-a805-233fda97c49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "# Print the parsed items\n",
    "print(items)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4f2c2-9142-4233-995d-37af093ad53c",
   "metadata": {},
   "source": [
    "Example : extract medical terms from some text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "438bea9b-f3eb-4e40-9271-ef5b94349576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt asking for medical terms\n",
    "prompt = (\n",
    "    \"Extract the medical terms from the following text and provide them in CSV format:\\n\\n\"\n",
    "    \"The patient was diagnosed with hypertension and prescribed medication for managing high blood pressure. \"\n",
    "    \"The treatment plan includes regular check-ups, a balanced diet, and exercise to improve overall health.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "52196dcc-dd07-4721-a9bc-0a2c4fb3bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the language model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5305482a-a5a7-4bcc-a087-19ee2ff24e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response from the language model\n",
    "response = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "732cb016-e206-49c1-bb9a-7b848821bce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='hypertension, medication, high blood pressure, treatment plan, check-ups, balanced diet, exercise, overall health', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 56, 'total_tokens': 79, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cacb9c60-099b-4ffa-b019-d4fc300126e4-0', usage_metadata={'input_tokens': 56, 'output_tokens': 23, 'total_tokens': 79, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "50bbec53-e96a-4dd1-b2ca-048d075136d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the output parser\n",
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "23aa6b74-65c3-4a4d-b4ad-94d54c0d77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the response to extract medical terms\n",
    "medical_terms_csv = parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "acd3ce60-2c5c-458d-8059-427b36447312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hypertension', 'medication', 'high blood pressure', 'treatment plan', 'check-ups', 'balanced diet', 'exercise', 'overall health']\n"
     ]
    }
   ],
   "source": [
    "# Print the parsed medical terms in CSV format\n",
    "print(medical_terms_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3bc31-0f66-4e47-ad52-d24243f897f8",
   "metadata": {},
   "source": [
    "#### Assembling the Many Pieces of an LLM Application\n",
    "\n",
    "The key components you’ve learned about so far are essential building blocks of the LangChain framework. This leads us to the critical question: \n",
    "\n",
    "- How Do You Combine Them Effectively to Build Your LLM Application?\n",
    "\n",
    "> RUNNABLE INTERFACE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b9890-a9c8-4be3-9b02-528764dd9f2e",
   "metadata": {},
   "source": [
    "#### Methods for Input Transformation in LangChain\n",
    "\n",
    "1. **invoke**\n",
    "   - Transforms a single input into an output.\n",
    "\n",
    "2. **batch**\n",
    "   - Efficiently transforms multiple inputs into multiple outputs.\n",
    "\n",
    "3. **stream**\n",
    "   - Streams output from a single input as it’s produced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "599d770f-cd55-484f-801a-acfdddfdf6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f86160fa-ef29-4496-880b-55dce4c39eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4472bfa2-877f-40e8-b9a3-581d42585737",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke('Hi there!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f65e94f1-50c8-4645-a38c-4a42af376ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I realize this is somewhat off-topic however I needed to ask. Does operating a well-established website like yours require a lot of work? I’m completely new to operating a blog however I do write in my journal everyday. I’d like to start a'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a52d05-1c0f-43ec-8f0b-b32cab932c31",
   "metadata": {},
   "source": [
    "**batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c7a0891c-29b1-4a76-b941-966732c167e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = model.batch(['Hi there!', 'Bye!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "30a5af6b-ae57-4338-9fa1-7c405a628be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" I'm Ryan, a digital nomad and freelance writer based in Asia. I've been living and working remotely for over three years now, and it has been an incredible experience. I've been able to travel to new places, meet interesting people, and learn about different cultures all while earning a living.\\n\\nThere are a few key things that have helped me make the most of my digital nomad lifestyle. Here are my top tips for anyone who is interested in becoming a digital nomad:\\n\\n1. Start with a solid skill set\\n\\nBefore you can become a successful digital nomad, you need to have a skill or set of skills that you can offer remotely. This could be anything from writing and graphic design to web development or social media management. Take some time to assess your strengths and interests and determine what skills you can develop that are in demand in the remote job market.\\n\\n2. Build a strong online presence\\n\\nHaving a strong online presence is crucial for digital nomads. It's important to have a professional website, active social media accounts, and a strong portfolio showcasing your work. This will not only help you attract potential clients, but also establish credibility and build your personal brand.\\n\\n3. Network, network, network\\n\\nNetworking is key for digital nomads. Attend industry\",\n",
       " '\\n\\nGoodbye! Have a great day!']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3b5f80e8-be0a-46ac-9897-f2e207f3f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hi there!\n",
      "Completion:  I'm Ryan, a digital nomad and freelance writer based in Asia. I've been living and working remotely for over three years now, and it has been an incredible experience. I've been able to travel to new places, meet interesting people, and learn about different cultures all while earning a living.\n",
      "\n",
      "There are a few key things that have helped me make the most of my digital nomad lifestyle. Here are my top tips for anyone who is interested in becoming a digital nomad:\n",
      "\n",
      "1. Start with a solid skill set\n",
      "\n",
      "Before you can become a successful digital nomad, you need to have a skill or set of skills that you can offer remotely. This could be anything from writing and graphic design to web development or social media management. Take some time to assess your strengths and interests and determine what skills you can develop that are in demand in the remote job market.\n",
      "\n",
      "2. Build a strong online presence\n",
      "\n",
      "Having a strong online presence is crucial for digital nomads. It's important to have a professional website, active social media accounts, and a strong portfolio showcasing your work. This will not only help you attract potential clients, but also establish credibility and build your personal brand.\n",
      "\n",
      "3. Network, network, network\n",
      "\n",
      "Networking is key for digital nomads. Attend industry\n",
      "\n",
      "Input: Bye!\n",
      "Completion: \n",
      "\n",
      "Goodbye! Have a great day!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the completions for each input\n",
    "for input_text, completion in zip(['Hi there!', 'Bye!'], completions):\n",
    "    print(f\"Input: {input_text}\\nCompletion: {completion}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551214b-f9a2-402a-84db-169f7c73f0e7",
   "metadata": {},
   "source": [
    "`model.batch`: This method takes a list of input strings and processes them in a single call, allowing for efficient handling of multiple requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b9f0e791-cf2f-439f-9951-7701a502ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Good\n",
      "bye\n",
      "!\n",
      " Take\n",
      " care\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in model.stream('Bye!'): \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9021b-3bbb-4c46-b93a-355867edc262",
   "metadata": {},
   "source": [
    "`model.stream`: This method takes a single input string and produces output tokens incrementally. This is useful for applications where you want to display the output in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c3b7d-05a0-4bf0-8086-13d85511c03b",
   "metadata": {},
   "source": [
    "**Example :** Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4c07d48d-6624-4189-9539-863cd3f3c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ec712f73-15bc-40c7-a516-123154c52cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of questions\n",
    "questions = [\n",
    "    \"What are the symptoms of COVID-19?\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"What is the process of evolution?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1693aa3e-4d25-43be-a2b7-13a95492a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch to get answers for all questions\n",
    "answers = model.batch(questions)\n",
    "\n",
    "# Use batch to get answers for all questions, limiting responses to 3 sentences\n",
    "answers = model.batch([f\"Answer in 3 sentences max: '{question}'\" for question in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ec3e2002-2195-4b0c-a75f-ca2772c09f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the symptoms of COVID-19?\n",
      "Answer: Symptoms of COVID-19 include fever, cough, and shortness of breath. Other symptoms may include fatigue, body aches, and loss of taste or smell. If you experience any of these symptoms, it is important to get tested and follow public health guidelines.\n",
      "\n",
      "Question: How does photosynthesis work?\n",
      "Answer: Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight into energy. This energy is used to convert carbon dioxide and water into glucose and oxygen. The chlorophyll in the plant's cells captures sunlight, which then triggers a series of chemical reactions that ultimately produce glucose as a source of food for the plant.\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Answer: The capital of France is Paris. It is known for its iconic landmarks such as the Eiffel Tower and Louvre Museum. Paris is also famous for its art, fashion, and cuisine.\n",
      "\n",
      "Question: Explain the theory of relativity.\n",
      "Answer: The theory of relativity, developed by Albert Einstein, states that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is constant. It also shows that time can be perceived differently depending on the observer's relative motion. The theory has had a profound impact on our understanding of space, time, and gravity.\n",
      "\n",
      "Question: What is the process of evolution?\n",
      "Answer: Evolution is the process by which species change over time through the accumulation of genetic variations. These variations can occur through mutation, genetic drift, gene flow, and natural selection. Ultimately, evolution leads to the adaptation of species to their environments, resulting in the diversity of life on Earth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print each question with its corresponding answer\n",
    "for question, answer in zip(questions, answers):\n",
    "    print(f\"Question: {question}\\nAnswer: {(answer.content)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9933041-6253-4801-916e-078d7fc99efa",
   "metadata": {},
   "source": [
    "#### Benefits of Using `batch()`:\n",
    "\n",
    "- **Efficiency**: It processes multiple requests in parallel, reducing overall latency and improving response times.\n",
    "\n",
    "- **Single API Call**: Instead of making multiple sequential API calls, `batch()` combines all inputs into one API call, saving time and resources.\n",
    "\n",
    "- **Simplified Code**: By handling all inputs and outputs at once, `batch()` minimizes the need for writing multiple API requests, leading to cleaner and more maintainable code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b955c-99c4-41eb-a0f0-64ba06ad571f",
   "metadata": {},
   "source": [
    "#### Key Differences Between Loop and batch():\n",
    "- Loop: Would make one API call for each question.\n",
    "- Batch: Sends all questions in a single API call, processes them, and returns all the answers together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604100c-3f52-49d3-98a6-a3d61b73b6a1",
   "metadata": {},
   "source": [
    "Another example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "61fcb0ef-a1a3-4b5f-9f38-23f0ec7bd487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of articles (text snippets)\n",
    "articles = [\n",
    "    \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines.\",\n",
    "    \"Machine learning is a subset of AI that involves the use of algorithms to analyze data.\",\n",
    "    \"Deep learning is a type of machine learning that uses neural networks to model complex patterns.\",\n",
    "    \"Natural language processing (NLP) allows computers to understand and respond to human language.\",\n",
    "    \"Reinforcement learning is an area of machine learning concerned with how agents ought to take actions in an environment.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7b825b10-7773-48d1-b44f-3a00d55de52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch to summarize articles with a sentence limit of 3 sentences\n",
    "summaries = model.batch([f\"Summarize the following article in 2 sentences max: '{article}'\" for article in articles])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "aa77e7ee-f4d3-48f0-935b-662302a2ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Artificial intelligence (AI) refers to the simulation of human intelligence in machines.\n",
      "Summary: Artificial intelligence (AI) is the replication of human intelligence in machines, allowing them to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n",
      "\n",
      "Article: Machine learning is a subset of AI that involves the use of algorithms to analyze data.\n",
      "Summary: Machine learning is a type of artificial intelligence that uses algorithms to analyze data. It is a powerful tool for making predictions and decisions based on patterns in the data.\n",
      "\n",
      "Article: Deep learning is a type of machine learning that uses neural networks to model complex patterns.\n",
      "Summary: Deep learning is a form of machine learning that utilizes neural networks to analyze intricate patterns. It is particularly effective in tasks such as image and speech recognition.\n",
      "\n",
      "Article: Natural language processing (NLP) allows computers to understand and respond to human language.\n",
      "Summary: Natural language processing (NLP) enables computers to interpret and generate human language, improving communication between machines and people.\n",
      "\n",
      "Article: Reinforcement learning is an area of machine learning concerned with how agents ought to take actions in an environment.\n",
      "Summary: Reinforcement learning is a subset of machine learning that focuses on how agents should make decisions in an environment. It involves learning through trial and error to maximize rewards received for certain actions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print each article with its corresponding summary\n",
    "for article, summary in zip(articles, summaries):\n",
    "    print(f\"Article: {article}\\nSummary: {summary.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7273c3-4321-42e3-81a5-b80f3c0857a0",
   "metadata": {},
   "source": [
    "#### combine a chat model and a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "75f5e9d6-ff9b-4ad0-ac15-df0d008d7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7f944018-2ebb-4e0e-85b2-6d7480df4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template with a system message and a human input\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4f16b4e9-f1fe-48a6-944f-de9ceb826090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "07ba21ec-ea81-4d1f-8752-fe3c03dc5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a function using the @chain decorator\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    # Create the prompt by passing values into the template\n",
    "    prompt = template.invoke(values)\n",
    "    \n",
    "    # Use the chat model to invoke the prompt and return the result\n",
    "    return model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "231b0610-8ba2-41e7-b6c0-5c219a7d117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chatbot function by passing a question\n",
    "result = chatbot.invoke({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f05d8b4c-409c-4611-83ff-e0f57e886d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Many law schools offer LLM (Master of Laws) programs, including top law schools such as Harvard Law School, Yale Law School, Stanford Law School, Columbia Law School, and NYU School of Law. Additionally, many other law schools around the world also offer LLM programs, so there are numerous options available depending on your preferences and area of interest.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the result\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285373a9-cbd8-4164-ae27-a2d377faa9b6",
   "metadata": {},
   "source": [
    "#### Example: Multi-Step Knowledge Retrieval and Question Answering\n",
    "\n",
    "- The `@chain` decorator in LangChain allows us to create complex `pipelines` where different parts of the workflow (e.g., processing input, calling an LLM, post-processing) are handled sequentially in a reusable and composable way.\n",
    "\n",
    "- here we use @chain to build a chatbot that processes user questions, retrieves data from a mock knowledge base, and then passes the combined context and question to the LLM for a detailed answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ce6ea176-4343-4c96-95fb-8b2fef8c81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a2d755a4-d66c-4427-bdf0-be149abd0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define a mock function for retrieving knowledge base content\n",
    "def retrieve_knowledge_base(question):\n",
    "    knowledge_base = {\n",
    "        \"python\": \"Python is a versatile programming language used for web development, data science, and automation.\",\n",
    "        \"ai\":     \"Artificial intelligence (AI) is the simulation of human intelligence in machines designed to think and learn.\",\n",
    "        \"cloud\":  \"Cloud computing provides scalable computing resources over the internet, enabling flexible IT infrastructure.\"\n",
    "    }\n",
    "    return knowledge_base.get(question.lower(), \"No relevant information found in the knowledge base.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b58897b3-b563-4ddb-9482-36952098fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the prompt template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant with access to a knowledge base.'),\n",
    "    ('system', 'Knowledge base info: {context}'),\n",
    "    ('human',  '{question}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "58fc96be-b53c-4051-aace-62aa26ccf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the chat model\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f58ac9ec-3387-4cc4-a739-1344790af43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the chained function\n",
    "@chain\n",
    "def complex_chatbot(values):\n",
    "    # Step 5: Retrieve relevant knowledge from the mock knowledge base\n",
    "    context = retrieve_knowledge_base(values['question'])\n",
    "    \n",
    "    # Step 6: Pass the knowledge and question to the prompt template\n",
    "    prompt = template.invoke({\n",
    "        \"context\":  context,\n",
    "        \"question\": values['question']\n",
    "    })\n",
    "    \n",
    "    # Step 7: Call the model with the complete prompt\n",
    "    return model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a9491e5b-c43f-4a27-bdef-f91479d3bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Use the chatbot function\n",
    "result = complex_chatbot.invoke({\n",
    "    \"question\": \"Tell me about Potato\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ca4bec99-a2e1-48a1-b9cc-958b65c6a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potatoes are starchy tuberous crops that belong to the nightshade family. They are a widely consumed food crop around the world and are known for their versatility in cooking. Potatoes can be boiled, baked, fried, mashed, or used in various dishes such as soups, stews, and salads. They are a good source of vitamins, minerals, and dietary fiber. There are many different varieties of potatoes, each with its own unique flavor, texture, and ideal cooking method. Potatoes are also used to make popular products like French fries, potato chips, and vodka.\n"
     ]
    }
   ],
   "source": [
    "# Display the result\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0aceab-c114-48e6-8722-31c50e0d4959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
