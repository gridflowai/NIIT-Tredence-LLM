{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfea182-55fa-493b-90ce-4d3ddea5c0e2",
   "metadata": {},
   "source": [
    "#### **BERT Embeddings**\n",
    "\n",
    "- **Description**:  \n",
    "  BERT embeddings provide contextualized vector representations for text, enabling semantic understanding of sentences and words in context.\n",
    "\n",
    "- **Dataset Used**:  \n",
    "  Pretrained on the **BooksCorpus** and **English Wikipedia** (uncased).\n",
    "\n",
    "- **When to Use**:  \n",
    "  - Tasks requiring context-aware embeddings, such as text similarity, classification, or question answering.\n",
    "  - General-purpose semantic similarity tasks.\n",
    "\n",
    "- **Key Points**:  \n",
    "  - **Pros**: Captures contextual nuances; widely supported.  \n",
    "  - **Cons**: Computationally expensive; embeddings can be large.\n",
    "\n",
    "- **Python Code (Example Usage)**:\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example text\n",
    "text = \"Retrieval-Augmented Generation is an advanced AI technique.\"\n",
    "\n",
    "# Tokenize and encode\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the embeddings (last hidden state)\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings.shape)  # [batch_size, sequence_length, hidden_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7e655-a75f-4337-90dd-8e9b4f4e78eb",
   "metadata": {},
   "source": [
    "#### **Sentence-BERT (SBERT)**\n",
    "\n",
    "- **Description**:  \n",
    "  A variation of BERT fine-tuned specifically for producing sentence-level embeddings optimized for semantic similarity tasks.\n",
    "\n",
    "- **Dataset Used**:  \n",
    "  Pretrained on **SNLI (Stanford Natural Language Inference)** and **STS (Semantic Textual Similarity)** datasets.\n",
    "\n",
    "- **When to Use**:  \n",
    "  - Sentence-level tasks such as similarity search or clustering.  \n",
    "  - RAG systems requiring fast and efficient embeddings.\n",
    "\n",
    "- **Key Points**:  \n",
    "  - **Pros**: Produces compact embeddings; highly accurate for semantic tasks.  \n",
    "  - **Cons**: May not capture token-level nuances.\n",
    "\n",
    "- **Python Code (Example Usage)**:\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\"What is RAG?\", \"How does RAG work?\"]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)  # [number_of_sentences, embedding_dimension]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90138a7b-f6e3-4cf5-b736-f5701a715eed",
   "metadata": {},
   "source": [
    "#### **DPR (Dense Passage Retriever)**\n",
    "\n",
    "- **Description**:  \n",
    "  A dual-encoder model designed for retrieving relevant passages for a query in large-scale document retrieval systems.\n",
    "\n",
    "- **Dataset Used**:  \n",
    "  Pretrained on **Natural Questions** and **TriviaQA** datasets.\n",
    "\n",
    "- **When to Use**:  \n",
    "  - For retrieval tasks in RAG systems.  \n",
    "  - QA systems where query-passage similarity is crucial.\n",
    "\n",
    "- **Key Points**:  \n",
    "  - **Pros**: Excels in passage retrieval; works well with RAG pipelines.  \n",
    "  - **Cons**: Requires fine-tuning for domain-specific tasks.\n",
    "\n",
    "- **Python Code (Example Usage)**:\n",
    "```python\n",
    "from transformers import DPRQuestionEncoderTokenizer, DPRQuestionEncoder\n",
    "\n",
    "# Load DPR tokenizer and model\n",
    "tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "# Example query\n",
    "query = \"What is Retrieval-Augmented Generation?\"\n",
    "\n",
    "# Tokenize the query\n",
    "inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).pooler_output\n",
    "\n",
    "print(embeddings.shape)  # [batch_size, hidden_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024ec99-32b3-4bc5-b0e3-dda1dda23f31",
   "metadata": {},
   "source": [
    "| Feature                | **BERT**                                              | **SBERT**                                             | **DPR**                                               |\n",
    "|------------------------|-------------------------------------------------------|------------------------------------------------------|-------------------------------------------------------|\n",
    "| **Description**        | Contextualized embeddings for words in context.       | Sentence-level embeddings optimized for similarity.  | Dual-encoder model for passage retrieval tasks.       |\n",
    "| **Primary Use**        | Token-level understanding and contextual modeling.    | Semantic similarity, clustering, and search.         | Document/passage retrieval in QA and RAG systems.     |\n",
    "| **Training Dataset**   | BooksCorpus and English Wikipedia.                    | SNLI and STS datasets.                               | Natural Questions and TriviaQA.                       |\n",
    "| **Output Type**        | Word-level embeddings for each token.                 | Single vector for an entire sentence.                | Single vector for queries or passages.                |\n",
    "| **Pros**               | Rich token-level context; versatile.                  | Compact embeddings; fast; highly accurate.           | Accurate retrieval; scales well with large datasets.  |\n",
    "| **Cons**               | Large embedding size; computationally expensive.      | Lacks token-level nuance; domain-specific fine-tuning may be needed. | Requires domain-specific fine-tuning.                 |\n",
    "| **Best Fit For**       | Language modeling, classification, and token tagging. | Sentence similarity, clustering, and ranking tasks.  | RAG pipelines and passage-based QA systems.           |\n",
    "| **Example Model**      | `bert-base-uncased`                                   | `all-MiniLM-L6-v2`                                   | `facebook/dpr-question_encoder-single-nq-base`        |\n",
    "| **Embedding Size**     | [Batch Size, Sequence Length, Hidden Size]            | [Batch Size, Embedding Size]                         | [Batch Size, Hidden Size]                             |\n",
    "| **Computational Cost** | High                                                  | Moderate                                             | Moderate                                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b040b-4430-4c70-b170-67ddae2c324b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
