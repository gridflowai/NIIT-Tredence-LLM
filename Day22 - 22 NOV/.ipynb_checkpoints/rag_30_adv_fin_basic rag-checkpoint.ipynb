{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a06a4a8-8ff5-4ea0-9a7f-44455d4bd8cb",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "#### Retrievals using embeddings (key points)\n",
    "-----------------------------------\n",
    "\n",
    "**Dataset**\n",
    "- PDF file on MS annual report (2022)\n",
    "    - plain text\n",
    "    - tables\n",
    "    - Plots/graphs\n",
    "    \n",
    "**Extract data**\n",
    "- Extract text pages\n",
    "\n",
    "**Chunking**\n",
    "- Two-Step Chunking Strategy: LangChain + SentenceTransformersTokenTextSplitter\n",
    "\n",
    "**Embeddings**\n",
    "- sentence transformer (from chromadb)\n",
    "\n",
    "**Vector database**\n",
    "- in memory chromadb\n",
    "\n",
    "**Query**\n",
    "- Using embeddings\n",
    "- Using RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb3b2f6-6609-4d65-a019-9f368dda92ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3ae9d0-0e4a-4ac2-9ec5-e66617ae8933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62aa28f5-20c2-495f-a11a-9c7e086b64bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pdfreader\n",
    "#!pip install PypDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9531164-80c9-4121-9435-c91cef34e723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da377cd-96a6-4e96-a370-cc924358f218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper_utils import word_wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ab272c-88d1-444c-ae93-37fa27f01a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06fe869-b8cc-4a56-85dd-1db4f6697b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = PdfReader(r\".\\data\\microsoft_annual_report_2022.pdf\")\n",
    "pdf_texts = [p.extract_text().strip() for p in reader.pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec75a6d0-42f2-4189-acef-e2ef6524812f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39bd6463-cd0e-4b5e-8903-36f840fc5ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the empty strings\n",
    "pdf_texts = [text for text in pdf_texts if text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35306d0a-dcd0-4a5b-8531-73f7711f6d98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049a5de-8b85-49c3-95d6-a7289ddc27b2",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter**\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` is a utility that helps split long text into smaller chunks while maintaining as much context as possible. Here's how it works:\n",
    "\n",
    "##### Separators\n",
    "\n",
    "The `separators` list defines the order in which the text will be split. In this example:\n",
    "\n",
    "- It first attempts to split by two newlines (`\"\\n\\n\"`), which typically indicates a paragraph break.\n",
    "- If the chunk size condition isn't met, it moves on to split by a single newline (`\"\\n\"`), indicating line breaks or new sentences.\n",
    "- Then it tries to split by period followed by a space (`\". \"`), which indicates sentence boundaries.\n",
    "- After that, it splits by a space (`\" \"`), which breaks the text at the word level.\n",
    "- Finally, it splits by individual characters (`\"\"`) if none of the above yield a chunk that meets the size requirement.\n",
    "\n",
    "##### Chunk size and overlap\n",
    "\n",
    "- `chunk_size=1000` means that each chunk will have a maximum of 1000 characters.\n",
    "- `chunk_overlap=0` means there will be no overlap between consecutive chunks (i.e., no repeated content).\n",
    "\n",
    "##### Recursive splitting\n",
    "\n",
    "The process is recursive because it starts from the largest separator (paragraphs), and if the resulting chunk is still larger than 1000 characters, it moves down to the next smaller separator (sentences, words, etc.), ensuring that the chunks are as close to 1000 characters as possible while retaining coherent pieces of text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b401d2-56bc-4b4c-ae88-0612b921ec4a",
   "metadata": {},
   "source": [
    "```python\n",
    "text = \"This is a long paragraph with multiple sentences. It discusses several topics and ideas, flowing continuously. For instance, it talks about machine learning, deep learning, and various AI applications. While doing so, it doesn’t include paragraph breaks or line breaks. Everything is packed in a single block.\"\n",
    "```\n",
    "\n",
    "##### Initial Split\n",
    "It tries to split using `\"\\n\\n\"` (paragraph breaks). There are no `\\n\\n` in this text, so no split happens.\n",
    "\n",
    "##### Next Split\n",
    "It then looks for `\"\\n\"` (line breaks). There are none here either.\n",
    "\n",
    "##### Next Split\n",
    "It tries `\". \"` (sentence breaks). Here, it successfully splits the text into three sentences:\n",
    "- \"This is a long paragraph with multiple sentences.\"\n",
    "- \"It discusses several topics and ideas, flowing continuously.\"\n",
    "- \"For instance, it talks about machine learning, deep learning, and various AI applications.\"\n",
    "- \"While doing so, it doesn’t include paragraph breaks or line breaks. Everything is packed in a single block.\"\n",
    "\n",
    "##### Final Chunks\n",
    "If any of these sentences exceed 1000 characters, it continues splitting by `\" \"` (spaces) and eventually by characters if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5e138-bd01-4f6b-bce9-e0d708390a23",
   "metadata": {},
   "source": [
    "```python\n",
    "text = \"\"\"Data science is an interdisciplinary field that uses various techniques to extract insights from data. It involves statistics, machine learning, and data analysis.\n",
    "\n",
    "Machine learning is a subset of AI that enables systems to learn from data and improve from experience.\n",
    "\n",
    "Deep learning, a branch of machine learning, uses neural networks to model complex patterns in data.\"\"\"\n",
    "```\n",
    "\n",
    "##### Initial Split\n",
    "The first separator `\"\\n\\n\"` (paragraph breaks) will be applied:\n",
    "- \"Data science is an interdisciplinary field that uses various techniques to extract insights from data. It involves statistics, machine learning, and data analysis.\"\n",
    "- \"Machine learning is a subset of AI that enables systems to learn from data and improve from experience.\"\n",
    "- \"Deep learning, a branch of machine learning, uses neural networks to model complex patterns in data.\"\n",
    "\n",
    "##### Next Split\n",
    "If any paragraph exceeds 1000 characters, it would then try to split further using `\"\\n\"`, `\". \"`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a205daa-c3d6-4c66-a2a6-79c10bf4b749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80507da8-2fdf-40df-b42c-6303de07277e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators   = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size   = 1000,\n",
    "    chunk_overlap= 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2aaec2b-1d2d-473f-95a4-f26777ee1f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e672aa-4b52-4c06-ba44-8847cba3aa93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 344\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6459ebb-787e-4177-9b04-89065fb652ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increased, due in large part to significant global datacenter\\nexpansions and the growth in Xbox sales and usage. Despite \\nthese\\nincreases, we remain dedicated to achieving a net -zero future. We\\nrecognize that progress won’t always be linear, \\nand the rate at which\\nwe can implement emissions reductions is dependent on many factors that\\ncan fluctuate over time.  \\nOn the path to becoming water positive, we\\ninvested in 21 water replenishment projects that are expected to\\ngenerate \\nover 1.3 million cubic meters of volumetric benefits in nine\\nwater basins around the world. Progress toward our zero waste\\n\\ncommitment included diverting more than 15,200 metric tons of solid\\nwaste otherwise headed to landfills and incinerators, \\nas well as\\nlaunching new Circular Centers to increase reuse and reduce e-waste at\\nour datacenters.  \\nWe contracted to protect over 17,000 acres of land\\n(50% more than the land we use to operate), thus achieving our'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_wrap(character_split_texts[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56e427-cf55-46c6-9acd-ab08241e4adc",
   "metadata": {},
   "source": [
    "#### The SentenceTransformersTokenTextSplitter\n",
    "\n",
    "The `SentenceTransformersTokenTextSplitter` is designed to split text based on token count, using tokenization principles similar to those employed by models like Sentence Transformers. Here's how it works:\n",
    "\n",
    "##### chunk_overlap=0\n",
    "This means there is no overlap between consecutive chunks. Each chunk will be entirely separate from the previous one, with no repeated content.\n",
    "\n",
    "##### tokens_per_chunk=256\n",
    "This indicates that each chunk will contain a maximum of 256 tokens. Tokens here refer to the processed units of text after tokenization, which could be words, parts of words, punctuation marks, etc., depending on the tokenizer.\n",
    "\n",
    "##### Use Case\n",
    "This splitter is typically useful when working with models that have token limits (like many transformer models), where you need to control the number of tokens being processed at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de69881-5236-4bbc-bae2-7a32073f7128",
   "metadata": {},
   "source": [
    "#### Two-Step Chunking Strategy: LangChain + SentenceTransformersTokenTextSplitter\n",
    "\n",
    "Chunking first with LangChain using a chunk size of 1000 characters, followed by further splitting each of those chunks using `SentenceTransformersTokenTextSplitter` with 256 tokens, provides a `layered approach` to ensure efficient processing for large language models. Here's how it benefits:\n",
    "\n",
    "##### 1. Balanced Chunk Sizes for Text Processing\n",
    "- **Initial Character-Based Chunking:** The initial chunking by LangChain (1000 characters) ensures that the text is divided into manageable pieces that retain context, such as paragraphs or sentences, without breaking down into excessively small parts.\n",
    "- **Token-Based Splitting for Model Constraints:** After chunking, each chunk is split further based on token limits (256 tokens per chunk) to fit within the constraints of transformer models, preventing errors during inference.\n",
    "\n",
    "##### 2. Optimized for Transformer Models\n",
    "- Transformer-based models typically have a **maximum token limit** (often 512 or 1024 tokens). By splitting into 256-token chunks, you ensure that each chunk is well within the limit, reducing the risk of truncation or cutting off important information in the middle of a chunk.\n",
    "\n",
    "##### 3. Combines Flexibility with Granularity\n",
    "- **Character-based Splitting:** Handles initial splitting by context (paragraphs, sentences) and ensures that large blocks of text are broken up in a logical way without splitting mid-word.\n",
    "- **Token-based Splitting:** Offers more **granularity** by ensuring each piece fits neatly into a model’s processing window, providing efficient model performance without losing coherence.\n",
    "\n",
    "##### 4. Improved Performance for Downstream Tasks\n",
    "- The combination of these two splitting strategies helps to balance **context retention** (larger chunks from character splitting) with **computational efficiency** (smaller chunks optimized for transformer models).\n",
    "- This is especially useful for **tasks like text embedding, summarization, and question answering**, where the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cae71751-552d-4a47-bd22-73a579cb8521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095c095c-370e-4948-886e-66f0ce3b6581",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increased, due in large part to significant global datacenter\n",
      "expansions and the growth in xbox sales and usage. despite these\n",
      "increases, we remain dedicated to achieving a net - zero future. we\n",
      "recognize that progress won ’ t always be linear, and the rate at which\n",
      "we can implement emissions reductions is dependent on many factors that\n",
      "can fluctuate over time. on the path to becoming water positive, we\n",
      "invested in 21 water replenishment projects that are expected to\n",
      "generate over 1. 3 million cubic meters of volumetric benefits in nine\n",
      "water basins around the world. progress toward our zero waste\n",
      "commitment included diverting more than 15, 200 metric tons of solid\n",
      "waste otherwise headed to landfills and incinerators, as well as\n",
      "launching new circular centers to increase reuse and reduce e - waste\n",
      "at our datacenters. we contracted to protect over 17, 000 acres of land\n",
      "( 50 % more than the land we use to operate ), thus achieving our\n",
      "\n",
      "Total chunks: 349\n"
     ]
    }
   ],
   "source": [
    "token_split_texts = []\n",
    "\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dffe86e3-41cf-4fdd-a37c-04e923a476cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# essentially a BERT model\n",
    "# BERT gives embeddings for tokens\n",
    "# Sentence transformers take the tokens of a document and produces\n",
    "# embeddings for the documents (in this case chunks)\n",
    "\n",
    "# that is why we used sentence transformer token text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3546eab-9b26-41cd-97ad-bed62c629fca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "len(embedding_function([token_split_texts[10]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7e855-9415-470c-b872-d6c1484da13c",
   "metadata": {},
   "source": [
    "#### Overview of `SentenceTransformerEmbeddingFunction`\n",
    "\n",
    "1. **Input Text**: \n",
    "   - The function takes input text, which can be sentences, paragraphs, or even entire documents.\n",
    "\n",
    "2. **Tokenization**: \n",
    "   - It processes the text through a Sentence Transformer model, which first tokenizes the input to handle it appropriately for embedding.\n",
    "\n",
    "3. **Embedding Generation**: \n",
    "   - The tokenized input is passed through the model to generate embeddings. Each embedding is typically a fixed-length vector that represents the semantic meaning of the text.\n",
    "\n",
    "##### Key Parameters (Example)\n",
    "\n",
    "While the implementation specifics may vary, common parameters for initializing a SentenceTransformer embedding function might include:\n",
    "\n",
    "- **model_name**: The name of the pre-trained Sentence Transformer model to use (e.g., `\"all-MiniLM-L6-v2\"`).\n",
    "- **device**: Specifies whether to run the model on CPU or GPU for faster processing.\n",
    "\n",
    "##### Benefits\n",
    "\n",
    "1. **High-Quality Embeddings**: \n",
    "   - Sentence Transformers are pre-trained on large datasets and are optimized for producing high-quality embeddings that capture nuanced semantic meanings.\n",
    "\n",
    "2. **Versatility**: \n",
    "   - The embeddings can be used in various NLP applications, including:\n",
    "   - **Semantic similarity**\n",
    "   - **Information retrieval**\n",
    "   - **Text classification**\n",
    "   - **Clustering**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85735a8a-5f51-49c9-85e0-2f34bc8e7c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the client with allow_reset enabled\n",
    "chroma_client = chromadb.Client(\n",
    "    chromadb.config.Settings(\n",
    "        allow_reset      =True,                            # Enable the ability to reset the database\n",
    "        #persist_directory=\"path_to_persistent_directory\",  # Optional for persistence\n",
    "        #chroma_db_impl   =\"duckdb+parquet\"                 # Optional for persistence\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7101c191-91cc-4819-b84e-ef757caacace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database has been reset.\n"
     ]
    }
   ],
   "source": [
    "# reset\n",
    "chroma_client.reset()                # Clears all data in the database\n",
    "print(\"Database has been reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8448858b-abcc-44fa-b6dd-6d748ee33bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_collection = chroma_client.get_or_create_collection(\"microsoft_annual_report_2022\", \n",
    "                                                            embedding_function = embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "316d4ab8-b55e-45ab-93be-bc0195731ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 24.8 s\n",
      "Wall time: 22.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# takes time\n",
    "\n",
    "# create IDs\n",
    "ids = [str(i) for i in range(len(token_split_texts))]\n",
    "\n",
    "# add documents to chroma collection\n",
    "chroma_collection.add(ids=ids, documents=token_split_texts)\n",
    "\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "165dfe20-78b6-4abe-ac90-96ddde3e5c41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 note 13 — unearned revenue unearned revenue by segment was as\n",
      "follows : ( in millions ) june 30, 2022 2021 productivity and business\n",
      "processes $ 24, 558 $ 22, 120 intelligent cloud 19, 371 17, 710 more\n",
      "personal computing 4, 479 4, 311 total $ 48, 408 $ 44, 141 changes in\n",
      "unearned revenue were as follows : ( in millions ) year ended june 30,\n",
      "2022 balance, beginning of period $ 44, 141 deferral of revenue 110,\n",
      "455 recognition of unearned revenue ( 106, 188 ) balance, end of period\n",
      "$ 48, 408 revenue allocated to remaining performance obligations, which\n",
      "includes unearned revenue and amounts that will be invoiced and\n",
      "recognized as revenue in future periods, was $ 193 billion as of june\n",
      "30, 2022, of which $ 189 billion is related to the commercial portion\n",
      "of revenue. we expect to recognize approximately 45 % of this revenue\n",
      "over the next 12 months and the remainder thereafter. note 14 — leases\n",
      "\n",
      "\n",
      "that are not sold separately. • we tested the mathematical accuracy of\n",
      "management ’ s calculations of revenue and the associated timing of\n",
      "revenue recognized in the financial statements.\n",
      "\n",
      "\n",
      "82 in addition, certain costs incurred at a corporate level that are\n",
      "identifiable and that benefit our segments are allocated to them. these\n",
      "allocated costs include legal, including settlements and fines,\n",
      "information technology, human resources, finance, excise taxes, field\n",
      "selling, shared facilities services, and customer service and support.\n",
      "each allocation is measured differently based on the specific facts and\n",
      "circumstances of the costs being allocated. segment revenue and\n",
      "operating income were as follows during the periods presented : ( in\n",
      "millions ) year ended june 30, 2022 2021 2020 revenue productivity and\n",
      "business processes $ 63, 364 $ 53, 915 $ 46, 398 intelligent cloud 75,\n",
      "251 60, 080 48, 366 more personal computing 59, 655 54, 093 48, 251\n",
      "total $ 198, 270 $ 168, 088 $ 143, 015 operating income productivity\n",
      "and business processes $ 29, 687 $ 24, 351 $ 18, 724\n",
      "\n",
      "\n",
      "47 financial statements and supplementary data income statements ( in\n",
      "millions, except per share amounts ) year ended june 30, 2022 2021 2020\n",
      "revenue : product $ 72, 732 $ 71, 074 $ 68, 041 service and other 125,\n",
      "538 97, 014 74, 974 total revenue 198, 270 168, 088 143, 015 cost of\n",
      "revenue : product 19, 064 18, 219 16, 017 service and other 43, 586 34,\n",
      "013 30, 061 total cost of revenue 62, 650 52, 232 46, 078 gross margin\n",
      "135, 620 115, 856 96, 937 research and development 24, 512 20, 716 19,\n",
      "269 sales and marketing 21, 825 20, 117 19, 598 general and\n",
      "administrative 5, 900 5, 107 5, 111 operating income 83, 383 69, 916\n",
      "52, 959 other income, net 333 1, 186 77 income before income taxes 83,\n",
      "716 71, 102 53, 036 provision for income taxes 10, 978 9, 831 8, 755\n",
      "net income $ 72, 738 $ 61, 271 $ 44, 281\n",
      "\n",
      "\n",
      "intelligent cloud 32, 721 26, 126 18, 324 more personal computing 20,\n",
      "975 19, 439 15, 911 total $ 83, 383 $ 69, 916 $ 52, 959 no sales to an\n",
      "individual customer or country other than the united states accounted\n",
      "for more than 10 % of revenue for fiscal years 2022, 2021, or 2020.\n",
      "revenue, classified by the major geographic areas in which our\n",
      "customers were located, was as follows : ( in millions ) year ended\n",
      "june 30, 2022 2021 2020 united states ( a ) $ 100, 218 $ 83, 953 $ 73,\n",
      "160 other countries 98, 052 84, 135 69, 855 total $ 198, 270 $ 168, 088\n",
      "$ 143, 015 ( a ) includes billings to oems and certain multinational\n",
      "organizations because of the nature of these businesses and the\n",
      "impracticability of determining the geographic source of the revenue.\n",
      "revenue, classified by significant product and service offerings, was\n",
      "as follows : ( in millions ) year ended june 30, 2022 2021 2020\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What was the total revenue?\"\n",
    "\n",
    "results = chroma_collection.query(query_texts= [query], \n",
    "                                  n_results  = 5)\n",
    "\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "for document in retrieved_documents:\n",
    "    print(word_wrap(document))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b24fbfb-a36f-4166-bc5f-1ed67e532ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3d369e5-1662-4126-a366-67a14867a4d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5db20e41-3e75-409b-8643-12287aba8b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "    \n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert financial research assistant. Your users are asking questions about information contained in an annual report.\"\n",
    "            \"You will be shown the user's question, and the relevant information from the annual report. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model   = model,\n",
    "        messages= messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6523814-776b-4c77-b7b2-762dd39ce1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total revenue was $198,270 million for the year ended June 30,\n",
      "2022.\n"
     ]
    }
   ],
   "source": [
    "output = rag(query=query, retrieved_documents=retrieved_documents)\n",
    "\n",
    "print(word_wrap(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8761121-6749-4d92-aa18-741f0ef20fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
