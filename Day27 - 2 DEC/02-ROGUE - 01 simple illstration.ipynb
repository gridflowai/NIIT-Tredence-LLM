{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ed95cd-e5b8-4c6b-a07b-f9aaef0ddc66",
   "metadata": {},
   "source": [
    "`seqeval` is a Python framework for sequence labeling evaluation. \n",
    "\n",
    "`seqeval` can evaluate the performance of chunking tasks such as `named-entity recognition`, `part-of-speech tagging`, `semantic role labeling` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a63bd52-72cb-4dbb-a9bc-bd3652be9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95064a79-bcb7-4016-b59f-1ccd7279bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1ee897-31ef-4d2a-83b7-db9e7a509110",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'],      ['B-PER', 'I-PER', 'O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba31094f-bfa7-4143-8bae-65a80a704c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a8f2db-5c77-4d1c-aed7-6e72394fd952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b59b6a9-9e80-4d92-80e0-c5ba9ded6f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3080df-e5ac-4c72-b692-64659df3c201",
   "metadata": {},
   "source": [
    "#### SeqEval: A python package for evaluating Seq2Seq models\n",
    "\n",
    "- Sequence Evaluate (SeqEval) is a python package that computes metrics useful for evaluating Seq2Seq models on multiple tasks such as: machine translation, dialogue response generation, and text summarization. There already exists many packages to compute those metrics, but SeqEval puts them all in one place, and allows you to compute them in two lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d54e0e0-81a9-4c22-a761-941750f39a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sequence-evaluate\n",
      "  Downloading sequence_evaluate-0.0.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading sequence_evaluate-0.0.3-py3-none-any.whl (7.2 kB)\n",
      "Installing collected packages: sequence-evaluate\n",
      "Successfully installed sequence-evaluate-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sequence-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377aab27-0098-42d0-95a7-96eb0b24f4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rogue\n",
      "  Downloading rogue-0.0.2.tar.gz (5.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: rogue\n",
      "  Building wheel for rogue (setup.py): started\n",
      "  Building wheel for rogue (setup.py): finished with status 'done'\n",
      "  Created wheel for rogue: filename=rogue-0.0.2-py3-none-any.whl size=7222 sha256=fb0c8dce842859770bd360061dc271e2f4a6531fbc487ba013026e084980c98b\n",
      "  Stored in directory: c:\\users\\bhupe\\appdata\\local\\pip\\cache\\wheels\\6e\\4d\\87\\c82eb52b617e6146533d5644d081bca15472f8d2592c0b6c80\n",
      "Successfully built rogue\n",
      "Installing collected packages: rogue\n",
      "Successfully installed rogue-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3737cef2-e8aa-4155-b728-2f6d337c46f0",
   "metadata": {},
   "source": [
    "#### rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7eb5908-d438-4222-8185-0840c012b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80c91020-2e85-4c21-abc9-5e3ee72ec561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6577f8-645e-4681-b298-eedf4d63fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02338f4-4ae4-4d51-9b1d-fc15e7cf0c22",
   "metadata": {},
   "source": [
    "The evaluator expects two python lists containing candidates (outputs generated by the model) and references (ground-truth data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11e81a1-0c8c-440e-8986-38efaea51053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your candidate summary and reference summaries (can be a list)\n",
    "candidate_summary   = \"The quick brown fox jumps over the lazy dog.\"\n",
    "reference_summaries = \"The dog is lazy. The fox is quick.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2f6ebe3-c433-4cb9-a653-8375bd53afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scorer object (optional: specify metrics and stemming)\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "827da271-e40f-4ad7-8ceb-7438c7f604ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE scores\n",
    "# scorer.score(target, prediction)\n",
    "scores = scorer.score(prediction=candidate_summary, target=reference_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94e15519-2e11-439b-b0b3-bf056022b50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.6666666666666666, recall=0.75, fmeasure=0.7058823529411765),\n",
       " 'rougeL': Score(precision=0.2222222222222222, recall=0.25, fmeasure=0.23529411764705882)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e0d87-4a7e-43b4-95ab-b402db21fdf6",
   "metadata": {},
   "source": [
    "... example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f4c5e15-1e6d-42b7-8a91-4c16962b740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3738a92d-175a-4320-9530-d9a2aaf2dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Rouge scores for reference and candidate.\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8aa3111d-98fe-4611-8c87-7aff6aa829f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765),\n",
       " 'rouge2': Score(precision=0.2857142857142857, recall=0.25, fmeasure=0.26666666666666666),\n",
       " 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471),\n",
       " 'rougeLsum': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a28925-3e5b-4df9-91c8-2d3efb744413",
   "metadata": {},
   "source": [
    "#### py-rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be7e1e4f-f3db-48cb-a8c0-8a9d802629fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py-rouge\n",
      "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: py-rouge\n",
      "Successfully installed py-rouge-1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install py-rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72e9b18a-43d1-41ee-ab5f-b70252034be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b12d675-e1b7-4703-acbc-f49b83849ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(m, p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(m, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e7e7122-cb88-4b81-9909-00de2b24993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with Avg\n",
      "\trouge-1:\tP: 28.62\tR: 26.46\tF1: 27.49\n",
      "\trouge-2:\tP:  4.21\tR:  3.92\tF1:  4.06\n",
      "\trouge-3:\tP:  0.80\tR:  0.74\tF1:  0.77\n",
      "\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-l:\tP: 30.52\tR: 28.57\tF1: 29.51\n",
      "\trouge-w:\tP: 15.85\tR:  8.28\tF1: 10.87\n",
      "\n",
      "Evaluation with Best\n",
      "\trouge-1:\tP: 30.44\tR: 28.36\tF1: 29.37\n",
      "\trouge-2:\tP:  4.74\tR:  4.46\tF1:  4.59\n",
      "\trouge-3:\tP:  1.06\tR:  0.98\tF1:  1.02\n",
      "\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-l:\tP: 31.54\tR: 29.71\tF1: 30.60\n",
      "\trouge-w:\tP: 16.42\tR:  8.82\tF1: 11.47\n",
      "\n",
      "Evaluation with Individual\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-1:\tP: 38.54\tR: 35.58\tF1: 37.00\n",
      "\tHypothesis #0 & Reference #1: \n",
      "\t\trouge-1:\tP: 45.83\tR: 43.14\tF1: 44.44\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-1:\tP: 15.05\tR: 13.59\tF1: 14.29\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-2:\tP:  7.37\tR:  6.80\tF1:  7.07\n",
      "\tHypothesis #0 & Reference #1: \n",
      "\t\trouge-2:\tP:  9.47\tR:  8.91\tF1:  9.18\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-3:\tP:  2.13\tR:  1.96\tF1:  2.04\n",
      "\tHypothesis #0 & Reference #1: \n",
      "\t\trouge-3:\tP:  1.06\tR:  1.00\tF1:  1.03\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #0 & Reference #1: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-l:\tP: 42.11\tR: 39.39\tF1: 40.70\n",
      "\tHypothesis #0 & Reference #1: \n",
      "\t\trouge-l:\tP: 46.19\tR: 43.92\tF1: 45.03\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-l:\tP: 16.88\tR: 15.50\tF1: 16.16\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-w:\tP: 22.27\tR: 11.49\tF1: 15.16\n",
      "\tHypothesis #0 & Reference #1: \n",
      "\t\trouge-w:\tP: 24.56\tR: 13.60\tF1: 17.51\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-w:\tP:  8.29\tR:  4.04\tF1:  5.43\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for aggregator in ['Avg', 'Best', 'Individual']:\n",
    "    print('Evaluation with {}'.format(aggregator))\n",
    "    apply_avg  = aggregator == 'Avg'\n",
    "    apply_best = aggregator == 'Best'\n",
    "\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "    \n",
    "    hypothesis_1 = \"King Norodom Sihanouk has declined requests to chair a summit of Cambodia 's top political leaders , saying the meeting would not bring any progress in deadlocked negotiations to form a government .\\nGovernment and opposition parties have asked King Norodom Sihanouk to host a summit meeting after a series of post-election negotiations between the two opposition groups and Hun Sen 's party to form a new government failed .\\nHun Sen 's ruling party narrowly won a majority in elections in July , but the opposition _ claiming widespread intimidation and fraud _ has denied Hun Sen the two-thirds vote in parliament required to approve the next government .\\n\"\n",
    "    references_1 = [\"Prospects were dim for resolution of the political crisis in Cambodia in October 1998.\\nPrime Minister Hun Sen insisted that talks take place in Cambodia while opposition leaders Ranariddh and Sam Rainsy, fearing arrest at home, wanted them abroad.\\nKing Sihanouk declined to chair talks in either place.\\nA U.S. House resolution criticized Hun Sen's regime while the opposition tried to cut off his access to loans.\\nBut in November the King announced a coalition government with Hun Sen heading the executive and Ranariddh leading the parliament.\\nLeft out, Sam Rainsy sought the King's assurance of Hun Sen's promise of safety and freedom for all politicians.\",\n",
    "                    \"Cambodian prime minister Hun Sen rejects demands of 2 opposition parties for talks in Beijing after failing to win a 2/3 majority in recent elections.\\nSihanouk refuses to host talks in Beijing.\\nOpposition parties ask the Asian Development Bank to stop loans to Hun Sen's government.\\nCCP defends Hun Sen to the US Senate.\\nFUNCINPEC refuses to share the presidency.\\nHun Sen and Ranariddh eventually form a coalition at summit convened by Sihanouk.\\nHun Sen remains prime minister, Ranariddh is president of the national assembly, and a new senate will be formed.\\nOpposition leader Rainsy left out.\\nHe seeks strong assurance of safety should he return to Cambodia.\\n\",\n",
    "                    ]\n",
    "    \n",
    "    hypothesis_2 = \"China 's government said Thursday that two prominent dissidents arrested this week are suspected of endangering national security _ the clearest sign yet Chinese leaders plan to quash a would-be opposition party .\\nOne leader of a suppressed new political party will be tried on Dec. 17 on a charge of colluding with foreign enemies of China '' to incite the subversion of state power , '' according to court documents given to his wife on Monday .\\nWith attorneys locked up , harassed or plain scared , two prominent dissidents will defend themselves against charges of subversion Thursday in China 's highest-profile dissident trials in two years .\\n\"\n",
    "    references_2 = \"Hurricane Mitch, category 5 hurricane, brought widespread death and destruction to Central American.\\nEspecially hard hit was Honduras where an estimated 6,076 people lost their lives.\\nThe hurricane, which lingered off the coast of Honduras for 3 days before moving off, flooded large areas, destroying crops and property.\\nThe U.S. and European Union were joined by Pope John Paul II in a call for money and workers to help the stricken area.\\nPresident Clinton sent Tipper Gore, wife of Vice President Gore to the area to deliver much needed supplies to the area, demonstrating U.S. commitment to the recovery of the region.\\n\"\n",
    "\n",
    "    all_hypothesis = [hypothesis_1, hypothesis_2]\n",
    "    all_references = [references_1, references_2]\n",
    "    \n",
    "    scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "            for hypothesis_id, results_per_ref in enumerate(results):\n",
    "                nb_references = len(results_per_ref['p'])\n",
    "                for reference_id in range(nb_references):\n",
    "                    print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
    "                    print('\\t' + prepare_results(metric,results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "            print()\n",
    "        else:\n",
    "            print(prepare_results(metric, results['p'], results['r'], results['f']))\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a85d7566-b923-4178-905b-122838f21310",
   "metadata": {},
   "source": [
    "Interpreting the Scores:\n",
    "\n",
    "The scores range from 0 to 1, with higher values indicating better quality summaries. Here's a general guideline:\n",
    "\n",
    "0.0 - 0.3: Low score, suggesting significant mismatch between the summary and references.\n",
    "0.3 - 0.6: Moderate score, indicating some overlap but room for improvement.\n",
    "0.6 - 0.8: High score, suggesting a good summary capturing essential information.\n",
    "0.8 - 1.0: Very high score, indicating a summary that closely resembles a human-written reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55f79e-2fac-4d12-bbde-46ac30a4146b",
   "metadata": {},
   "source": [
    "#### Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4043f7-4d88-4fdb-887d-57dce56d934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2447efae-8375-4cc0-9c26-6b89159d74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # api_key = api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ca874e-5102-42a9-895f-2469938ae33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. \n",
    "It is a set of metrics used to evaluate the quality of machine-generated text. \n",
    "ROUGE primarily compares the overlap of n-grams, sequences, or words between a generated text and a reference text. \n",
    "Common ROUGE variants include ROUGE-1, ROUGE-2, and ROUGE-L. \n",
    "ROUGE-1 measures unigram overlap, while ROUGE-2 measures bigram overlap. \n",
    "ROUGE-L captures the longest common subsequence between the texts. \n",
    "Higher ROUGE scores indicate better quality, with scores ranging from 0 to 1. \n",
    "ROUGE is widely used in tasks like summarization, machine translation, and text generation. \n",
    "Precision, recall, and F1-score are calculated for each ROUGE variant. \n",
    "Despite its popularity, ROUGE focuses on word overlap and may not capture semantic quality.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c0ff02-68e7-4460-83eb-dad90e8c169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference text (manually written summary)\n",
    "reference_text = \"\"\"ROUGE is a metric for evaluating text quality based on overlap with reference texts. \n",
    "Variants like ROUGE-1, ROUGE-2, and ROUGE-L measure unigram, bigram, and longest common subsequence overlaps. \n",
    "Itâ€™s commonly used in summarization and machine translation tasks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c5a891-06d4-43d1-811b-c8e411f7fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Summarize this text:\\n{original_text}\"}\n",
    "                ],\n",
    "                model=\"gpt-4o-mini\",\n",
    "                #max_tokens = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a55add83-5e9b-4f53-adcd-85379a27813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the generated text\n",
    "generated_text = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f9a332-e8f5-4c29-8eff-e8f5d752634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e615559-926c-4d32-b8ed-648849e4e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE scores between reference text and generated text\n",
    "scores = scorer.score(reference_text, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb70df79-4f61-4004-8a20-092d4b4a5fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Scores (Reference vs Generated):\n",
      "rouge1: Precision=0.3608, Recall=0.8537, F1=0.5072\n",
      "rouge2: Precision=0.1562, Recall=0.3750, F1=0.2206\n",
      "rougeL: Precision=0.2784, Recall=0.6585, F1=0.3913\n"
     ]
    }
   ],
   "source": [
    "# Display ROUGE scores\n",
    "print(\"\\nROUGE Scores (Reference vs Generated):\")\n",
    "for metric, score in scores.items():\n",
    "    print(f\"{metric}: Precision={score.precision:.4f}, Recall={score.recall:.4f}, F1={score.fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fd56e-6cc7-4769-b6a0-239e8946a16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
