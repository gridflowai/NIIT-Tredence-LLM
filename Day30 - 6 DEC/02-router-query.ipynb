{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7feb4dc8-87bf-47cd-b888-ca0d226de151",
   "metadata": {},
   "source": [
    "----------------------\n",
    "#### Router Engine\n",
    "----------------------\n",
    "- In LlamaIndex, a `RouterQueryEngine` (also known as a router query engine) allows you to dynamically route queries to different underlying query engines or indexes based on the type or content of the query.\n",
    "- This approach is useful when working with multiple types of data or specialized indexes where specific queries are best handled by specific engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e304aa-8014-45e0-bf6c-194201421acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12d8651-8d10-46f0-ad1a-1420b93c9139",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af6320-64d0-4396-8106-e1e17376a81d",
   "metadata": {},
   "source": [
    "#### asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99920974-1877-4873-b731-23b5f33c1aba",
   "metadata": {},
   "source": [
    "- Suppose we have an asynchronous function that makes two async calls:\n",
    "    - one to asyncio.sleep to simulate a delay, and\n",
    "    - another to print a message.\n",
    "- Weâ€™ll try running this function with and without `nest_asyncio` in an environment (like a Jupyter notebook) that already has an event loop running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1703985f-3d06-41c4-96c8-2f5114cbafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def fetch_data():\n",
    "    await asyncio.sleep(1)  # Simulates a delay (e.g., network call)\n",
    "    print(\"Data fetched!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e4b05-e39e-4c22-a48c-c4d388cd3f9b",
   "metadata": {},
   "source": [
    "`Without nest_asyncio`\n",
    "If you try to run the code below directly in a Jupyter notebook without nest_asyncio, you will likely see an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06cf3761-8127-46a1-999c-70d30ce47134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched!\n"
     ]
    }
   ],
   "source": [
    "await fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1cb0b-9569-4709-98ca-c17ee64d671c",
   "metadata": {},
   "source": [
    "looks like nest_asyncio is already set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d1e622-2ca3-4106-b0cb-d5d1d5027dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e541b95a-6cde-4515-8aee-7c83d14344be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca17205-52ad-4731-afd7-9a7dfafba122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Initialize the SentenceSplitter: The chunk_size parameter provided  \n",
    "# appropriate if you want to split documents into chunks of up to 1024 characters.\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "# Split the documents into nodes\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399ece3-a892-4113-9bb5-79f562832117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the nodes\n",
    "# for node in nodes:\n",
    "#     print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75bcd2dc-8131-4efe-bc13-5f4fa2e066eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ce373-dbf0-4c42-bd0b-3f4c239dc04f",
   "metadata": {},
   "source": [
    "#### Define Summary Index and Vector Index over the Same Data\n",
    "\n",
    "- create two types of indexes, `SummaryIndex` and `VectorStoreIndex`, from a collection of nodes. \n",
    "\n",
    "`SummaryIndex`\n",
    "- Purpose: SummaryIndex is designed to provide a `concise summary of the documents or nodes`. It typically captures the `main points or essence of the content`.\n",
    "\n",
    "`VectorStoreIndex`\n",
    "- Purpose: VectorStoreIndex is used to create a vector representation of the nodes, which is essential for performing `similarity searches`, `clustering`, or any task that involves comparing the content of the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e4d172-0549-4c19-9059-c8e83e2deb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index  = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20424e-533d-4395-afd1-96419d911cbe",
   "metadata": {},
   "source": [
    "#### Define Query Engines and Set Metadata\n",
    "\n",
    "`Summary Query Engine`\n",
    "\n",
    "- Purpose: This creates a query engine for the SummaryIndex that allows you to query the summarized content.\n",
    "- Parameters:\n",
    "    - `response_mode=\"tree_summarize\"`: This specifies how the responses should be generated. In this case, \"tree_summarize\" likely indicates that the engine will use a hierarchical or structured approach to generate summaries of the nodes. This can help in organizing and presenting the summarized information in a tree-like format.\n",
    "    - `use_async=True`: This enables asynchronous processing, which can improve performance when dealing with large datasets or when making multiple queries concurrently.\n",
    "\n",
    "- Usage: The summary_query_engine allows you to query the index and retrieve summaries based on the indexed content. It's useful for quickly obtaining a high-level overview or summary of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9eacb3b-c4b2-4c92-adcf-6588cbb75aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode= \"tree_summarize\",\n",
    "    use_async    = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e7ba6-64c3-458a-b1a0-7267d3da6efa",
   "metadata": {},
   "source": [
    "`Vector Index Engine`\n",
    "\n",
    "- Purpose: This creates a query engine for the VectorStoreIndex that allows you to perform operations like similarity searches or semantic queries on the vectorized nodes.\n",
    "- Parameters:\n",
    "    - No additional parameters are provided, so it uses default settings for the vector-based query engine.\n",
    "\n",
    "- Usage: The vector_query_engine enables you to search for nodes that are similar in meaning or context. It's useful for tasks like finding related documents, clustering, or retrieving information based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13a84d5a-3ef4-4450-93fb-5a66fe33ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb5d16-8fc2-4a76-b089-64625530ab0d",
   "metadata": {},
   "source": [
    "using `QueryEngineTool` from llama_index to create tools that facilitate interactions with the `SummaryIndex` and `VectorStoreIndex` through their respective query engines.\n",
    "\n",
    "**SummaryTool**\n",
    "\n",
    "- `Purpose`: This creates a QueryEngineTool specifically for interacting with the summary_query_engine. The tool is configured to handle summarization-related queries.\n",
    "- `Parameters`:\n",
    "    - `query_engine = summary_query_engine`:\n",
    "        - This specifies that the tool will use the summary_query_engine for handling queries.\n",
    "        - It leverages the summarization capabilities of the SummaryIndex.\n",
    "- `description`:\n",
    "    - Provides a brief description of the tool's purpose.\n",
    "    - In this case, it's described as being \"Useful for summarization questions related to MetaGPT.\"\n",
    "    - This description can help users understand when to use this tool.\n",
    "- `Usage`:\n",
    "    - The summary_tool can be used to answer questions that require summarizing content from the indexed nodes, making it helpful for quickly obtaining overviews or summaries related to MetaGPT.\n",
    "\n",
    "**VectorTool**\n",
    "\n",
    "- `Purpose`: This creates a QueryEngineTool for interacting with the vector_query_engine. It is designed to handle queries that require retrieving specific contextual information from the indexed data.\n",
    "- `Parameters`:\n",
    "    - `query_engine = vector_query_engine`\n",
    "        - Specifies that this tool uses the vector_query_engine for queries.\n",
    "        - This engine is based on the VectorStoreIndex, which means it can perform semantic similarity searches and retrieve contextually relevant information.\n",
    "    - `description`:\n",
    "        - Describes the tool's utility as \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
    "        - This helps clarify that the tool is suitable for detailed searches and context retrieval.\n",
    "    - `Usage`: The vector_tool is ideal for tasks where you need to find specific information or context within the MetaGPT content, leveraging the vectorized representation of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3577db-751f-43d2-a8c6-72dbb0fc2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine = summary_query_engine,\n",
    "    description  = (\"Useful for summarization questions related to MetaGPT\"),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine = vector_query_engine,\n",
    "    description  = (\"Useful for retrieving specific context from the MetaGPT paper.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637980a-9b62-41a5-9f25-93daae13d49a",
   "metadata": {},
   "source": [
    "`return_direct (bool, default = False)`\n",
    "\n",
    "- If set to True, the tool will return the query result directly.\n",
    "- This can be useful when you want the output without any additional formatting or post-processing.\n",
    "\n",
    "`resolve_input_errors (bool, default = True)`\n",
    "\n",
    "- If set to True, this option attempts to automatically handle common input errors in queries, making the tool more robust against malformed queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89ffe6-c323-4414-b842-175ef8c029d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae0f53-29bd-4ab9-823e-d1416df38e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4200c50e-9317-4f27-8904-0f16190eeee9",
   "metadata": {},
   "source": [
    "#### Define Router Query Engine\n",
    "\n",
    "- creating a `RouterQueryEngine` that combines multiple query engines (summary_tool and vector_tool) into a single interface. This allows you to route queries to the appropriate engine based on the nature of the query.\n",
    "\n",
    "- `RouterQueryEngine`\n",
    "    - Purpose: The RouterQueryEngine acts as a central query engine that can direct queries to different sub-engines based on certain criteria. It's useful when you have multiple tools or engines for different types of queries, and you want to automatically choose the best one for a given query.\n",
    "    - Usage: By using RouterQueryEngine, you can send queries to a single point of interaction, and it will internally decide which specific query engine (summary_tool or vector_tool) to use for each query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cdbdef0-e0ea-4b6a-910b-532b8919cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092b9f9-006f-49c1-97e2-325fbab3d955",
   "metadata": {},
   "source": [
    "`LLMSingleSelector`\n",
    "\n",
    "- Purpose: The LLMSingleSelector is a component that helps the RouterQueryEngine decide which query engine to use for a given query. It uses a language model (LLM) to make this selection.\n",
    "- Usage: LLMSingleSelector.from_defaults() creates a default selector that will utilize the LLM to choose the most appropriate tool from the available options (summary_tool or vector_tool). This allows for intelligent routing of queries based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276f5199-7cab-4b08-91ee-ee0ae2315459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.selectors import LLMSingleSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a24540-acae-4443-9dbc-906fd0ec6be8",
   "metadata": {},
   "source": [
    "`Creating the Router Query Engine`\n",
    "\n",
    "- Parameters:\n",
    "    - selector=LLMSingleSelector.from_defaults(): Specifies that the LLMSingleSelector will be used to route queries to the appropriate tool.\n",
    "    - query_engine_tools=[summary_tool, vector_tool]: Provides a list of query engine tools that the RouterQueryEngine can route queries to. In this case, it's the summary_tool and vector_tool.\n",
    "    - verbose=True: Enables verbose mode, which means the engine will print additional information during query processing. This can be useful for debugging or understanding how queries are routed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9714d59-a5c7-40c3-8e24-89c8c9036c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RouterQueryEngine(\n",
    "    selector           = LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools = [\n",
    "                            summary_tool,\n",
    "                            vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5780037e-6919-4ba5-8c0f-78fbf4552d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document, which aligns with the purpose of choice 1, as it is useful for summarization questions..\n",
      "\u001b[0mThe document introduces MetaGPT, a meta-programming framework designed for multi-agent collaboration utilizing large language models (LLMs). It addresses challenges in automated problem-solving by incorporating Standardized Operating Procedures (SOPs) to enhance the coherence and accuracy of task execution among agents. MetaGPT organizes agents into specialized roles, enabling efficient task decomposition and structured communication, which reduces errors and improves collaboration.\n",
      "\n",
      "The framework employs a unique communication protocol that utilizes structured outputs instead of natural language dialogue, minimizing ambiguities. Additionally, it features an executable feedback mechanism that allows agents to debug and refine code during runtime, significantly improving code generation quality.\n",
      "\n",
      "Experimental results demonstrate that MetaGPT achieves state-of-the-art performance on various software engineering benchmarks, outperforming existing frameworks in terms of executability, efficiency, and overall task completion rates. The document also discusses the implications of using SOPs in multi-agent systems and outlines future directions for enhancing the framework's capabilities, including self-improvement mechanisms and dynamic collaboration models.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83595155-0229-427c-98c4-bc0a53374164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4210d314-6d7f-4e24-b36e-13fb502bff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question pertains to specific mechanisms of information sharing among agents, which would likely be detailed in the MetaGPT paper..\n",
      "\u001b[0mAgents share information with other agents through a shared message pool. They publish structured messages in this pool, allowing all agents to access and exchange information directly. This mechanism enhances communication efficiency by enabling agents to retrieve relevant information without needing to inquire about other agents. Additionally, agents can utilize a subscription mechanism to follow task-related information based on their role profiles, ensuring they receive only pertinent updates while avoiding information overload.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do agents share information with other agents?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4befc3-bf73-43de-adac-4cdf9c98c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "query_engine = get_router_query_engine(\"metagpt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90dd41eb-3ab0-4091-b7ad-f7f2219cca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for specific results from an ablation study, which would require retrieving specific context from the MetaGPT paper..\n",
      "\u001b[0mThe ablation study results show that MetaGPT effectively addresses challenges related to using large language models (LLMs) for software generation. By focusing on specific tasks like requirement analysis and package selection, MetaGPT guides the thinking process of LLMs, reducing issues such as code hallucinations, incomplete implementations, missing dependencies, and undiscovered bugs. Additionally, MetaGPT utilizes a global message pool and a subscription mechanism to tackle information overload, ensuring that relevant information is prioritized and efficiently communicated.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b790d4-4aa0-4c0f-b581-ef8c0b4e6dcb",
   "metadata": {},
   "source": [
    "#### Summarization Prompts\n",
    "\n",
    "These prompts aim to provide high-level summaries or specific insights about sections within the `MetaGPT` paper:\n",
    "\n",
    "1. **Overview Summaries**\n",
    "   - \"Provide an executive summary of the MetaGPT paper.\"\n",
    "   - \"Summarize the main objectives and findings of the MetaGPT study.\"\n",
    "   - \"What are the primary contributions of MetaGPT according to the authors?\"\n",
    "   - \"Summarize the introduction and motivation for creating MetaGPT.\"\n",
    "\n",
    "2. **Technical Summaries**\n",
    "   - \"Summarize the architecture of MetaGPT, highlighting its core components.\"\n",
    "   - \"Provide a brief summary of the training methodologies used in MetaGPT.\"\n",
    "   - \"Summarize the different layers and attention mechanisms employed in MetaGPT.\"\n",
    "\n",
    "3. **Experiment and Results Summaries**\n",
    "   - \"Summarize the experiments conducted in the MetaGPT study and their outcomes.\"\n",
    "   - \"What were the key findings and metrics used to evaluate MetaGPT's performance?\"\n",
    "   - \"Summarize the results section, particularly focusing on how MetaGPT compares to other models.\"\n",
    "\n",
    "4. **Use-Case and Application Summaries**\n",
    "   - \"Summarize the primary use cases for MetaGPT as discussed in the paper.\"\n",
    "   - \"Summarize the potential limitations of MetaGPT for practical applications.\"\n",
    "\n",
    "5. **Future Directions and Conclusions**\n",
    "   - \"Summarize the conclusions drawn in the MetaGPT paper.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Question-Answering (QnA) Prompts\n",
    "\n",
    "These prompts are crafted to extract specific answers or detailed explanations based on user queries about the `MetaGPT` paper:\n",
    "\n",
    "1. **Basic Information**\n",
    "   - \"What is MetaGPT, and what problem does it aim to solve?\"\n",
    "   - \"Who are the authors of the MetaGPT paper, and which institutions supported this research?\"\n",
    "   - \"What are the main research questions addressed in MetaGPT?\"\n",
    "\n",
    "2. **Technical Details**\n",
    "   - \"How does MetaGPT differ architecturally from previous GPT models?\"\n",
    "   - \"What training data was used to train MetaGPT, and how was it curated?\"\n",
    "   - \"Can you describe the role of multi-head attention in MetaGPT's architecture?\"\n",
    "   - \"What are the specific loss functions and optimization techniques used in MetaGPT?\"\n",
    "\n",
    "3. **Experiments and Evaluation**\n",
    "   - \"What benchmarks were used to evaluate MetaGPT's performance?\"\n",
    "   - \"How does MetaGPT perform compared to other state-of-the-art models on the benchmarks?\"\n",
    "   - \"What experimental setups were used to test MetaGPT, and what were the key variables?\"\n",
    "   - \"How did MetaGPT handle language-specific tasks in the evaluation?\"\n",
    "\n",
    "4. **Practical Applications and Limitations**\n",
    "   - \"What are the primary applications for MetaGPT proposed in the paper?\"\n",
    "   - \"What limitations do the authors note for the MetaGPT model in real-world scenarios?\"\n",
    "   - \"How scalable is MetaGPT, according to the authors, and what are its resource requirements?\"\n",
    "\n",
    "5. **Future Directions**\n",
    "   - \"What future research directions are suggested for improving MetaGPT?\"\n",
    "   - \"Are there any planned updates or variations of MetaGPT to address specific limitations?\"\n",
    "   - \"How does MetaGPT's architecture support potential future improvements or modifications?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76c3f20f-c4ad-4302-b0d2-a64a65aa7a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for specific details about the training data and curation process, which would be found in the MetaGPT paper..\n",
      "\u001b[0mThe training data used to train MetaGPT consisted of 70 diverse software development tasks. These tasks were carefully curated to cover a range of scenarios and challenges within the software development domain. The dataset included detailed prompts and names for each task, with a focus on ensuring a comprehensive representation of software development requirements.\n"
     ]
    }
   ],
   "source": [
    "# QnA\n",
    "response = query_engine.query(\"What training data was used to train MetaGPT, and how was it curated??\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "560e404f-b3a1-4855-a342-edc2bd8e8994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question pertains to specific limitations and potential updates or variations of MetaGPT, which would require retrieving specific context from the MetaGPT paper..\n",
      "\u001b[0mThere are no specific mentions of planned updates or variations of MetaGPT to address specific limitations in the provided context information.\n"
     ]
    }
   ],
   "source": [
    "# QnA\n",
    "response = query_engine.query(\"Are there any planned updates or variations of MetaGPT to address specific limitations?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd2f58-d5ef-4742-b328-883e206ae6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
